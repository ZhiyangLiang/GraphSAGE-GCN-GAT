{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter # 也可直接调用nn.Parameter\n",
    "from torch.nn.modules.module import Module # 也可直接调用nn.Module\n",
    "\n",
    "# 注意：torch.FloatTensor生成的元素数值非常接近0;torch.Long生成的元素数值非常大\n",
    "class GraphConvolution(Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        # self.weight2 = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None) #\n",
    "        self.reset_parameters() # 此处表示生成变量后(即上面的语句运行后),将会进行变量初始化(即执行该语句)\n",
    "\n",
    "    def reset_parameters(self): # 经测试,重写可覆盖\n",
    "        stdv = 1/math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        # self.weight2.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support) # torch.spmm支持sparse在前,dense在后的矩阵乘法\n",
    "        if self.bias is not None:         # adj是稀疏矩阵\n",
    "            return output +self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__+\"(\"+str(self.in_features)+\"->\"+str(self.out_features)+\")\"\n",
    "\n",
    "# class Readout(Module):\n",
    "#     def __init__(self, in_features):\n",
    "#         super(Readout, self).__init__()\n",
    "#         self.in_features = in_features\n",
    "#         self.weight = Parameter(torch.FloatTensor(in_features, in_features))\n",
    "#         self.reset_parameters()\n",
    "#\n",
    "#     def reset_parameters(self):\n",
    "#         stdv = 1/math.sqrt(self.weight.size(1))\n",
    "#         self.weight.data.uniform_(-stdv, stdv)\n",
    "#\n",
    "#     def forward(self, input):\n",
    "#         input = torch.sum(input, 0) # 此处不确定\n",
    "#         return torch.mm(input, self.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        # self.gc2 = GraphConvolution(nhid, nhid)\n",
    "        self.dropout = dropout\n",
    "        # self.rd = Readout(nhid)\n",
    "        # self.fc1 = nn.Linear(nhid, 1)\n",
    "        # nn.init.kaiming_normal_(self.fc1.weight)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "        # x = F.relu(self.gc2(x, adj))\n",
    "        # x = F.dropout(x, self.dropout, training=self.training)\n",
    "        # x = F.relu(self.rd(x))\n",
    "        # x = self.fc(x)\n",
    "        # return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{9, 3, 6}\n",
      "{9: array([1., 0., 0.]), 3: array([0., 1., 0.]), 6: array([0., 0., 1.])}\n",
      "[[0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# 理解独热编码的过程：\n",
    "import numpy as np\n",
    "a = [3, 9, 6]\n",
    "b = set(a)\n",
    "print(b)\n",
    "c = {t: np.identity(len(b))[i, :] for i, t in enumerate(b)}\n",
    "print(c)\n",
    "d = np.array(list(map(c.get, a)), dtype=np.int32) # c.get为字典内置函数\n",
    "print(d)\n",
    "# print(map(c.get, a))\n",
    "# print(list(map(c.get, a)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "  (0, 118)\t1.0\n",
      "  (0, 125)\t1.0\n",
      "  (0, 176)\t1.0\n",
      "  (0, 252)\t1.0\n",
      "  (0, 351)\t1.0\n",
      "  (0, 456)\t1.0\n",
      "  (0, 507)\t1.0\n",
      "  (0, 521)\t1.0\n",
      "  (0, 619)\t1.0\n",
      "  (0, 648)\t1.0\n",
      "  (0, 698)\t1.0\n",
      "  (0, 702)\t1.0\n",
      "  (0, 734)\t1.0\n",
      "  (0, 845)\t1.0\n",
      "  (0, 902)\t1.0\n",
      "  (0, 1205)\t1.0\n",
      "  (0, 1209)\t1.0\n",
      "  (0, 1236)\t1.0\n",
      "  (0, 1352)\t1.0\n",
      "  (0, 1426)\t1.0\n",
      "  (1, 12)\t1.0\n",
      "  (1, 509)\t1.0\n",
      "  (1, 620)\t1.0\n",
      "  (1, 763)\t1.0\n",
      "  (1, 882)\t1.0\n",
      "  :\t:\n",
      "  (2706, 1253)\t1.0\n",
      "  (2706, 1266)\t1.0\n",
      "  (2706, 1314)\t1.0\n",
      "  (2706, 1350)\t1.0\n",
      "  (2706, 1383)\t1.0\n",
      "  (2706, 1423)\t1.0\n",
      "  (2707, 19)\t1.0\n",
      "  (2707, 67)\t1.0\n",
      "  (2707, 136)\t1.0\n",
      "  (2707, 304)\t1.0\n",
      "  (2707, 422)\t1.0\n",
      "  (2707, 564)\t1.0\n",
      "  (2707, 737)\t1.0\n",
      "  (2707, 774)\t1.0\n",
      "  (2707, 877)\t1.0\n",
      "  (2707, 1073)\t1.0\n",
      "  (2707, 1075)\t1.0\n",
      "  (2707, 1156)\t1.0\n",
      "  (2707, 1178)\t1.0\n",
      "  (2707, 1203)\t1.0\n",
      "  (2707, 1205)\t1.0\n",
      "  (2707, 1301)\t1.0\n",
      "  (2707, 1333)\t1.0\n",
      "  (2707, 1335)\t1.0\n",
      "  (2707, 1351)\t1.0\n",
      "(2708, 1433)\n"
     ]
    }
   ],
   "source": [
    "# 理解np.genfromtxt和sp.csr_matrix的处理：\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "idx_features_labels = np.genfromtxt(\"{}{}.content\".format(\"./cora/\", \"cora\"), dtype=np.dtype(str))\n",
    "# print(idx_features_labels)\n",
    "mid_idx_features_labels = idx_features_labels[:, 1:-1]\n",
    "after_idx_features_labels = np.zeros(mid_idx_features_labels.shape)\n",
    "for i in range(mid_idx_features_labels.shape[0]):\n",
    "    for j in range(mid_idx_features_labels.shape[1]):\n",
    "        after_idx_features_labels[i][j] = int(mid_idx_features_labels[i][j])\n",
    "features = sp.csr_matrix(after_idx_features_labels, dtype=np.float32)\n",
    "print(after_idx_features_labels)\n",
    "print(features)\n",
    "print(features.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "# 注意此处a中的元素为字符串，且直接在a上修改覆盖无法生效，故要创建另一个与a的shape一致的变量\n",
    "a = idx_features_labels[1:5, 1:5]\n",
    "b = np.zeros(a.shape)\n",
    "for i in range(a.shape[0]):\n",
    "    for j in range(a.shape[1]):\n",
    "        # a[i][j] = int(a[i][j])\n",
    "        b[i][j] = int(a[i][j])\n",
    "\n",
    "# print(a[0][0]-1)\n",
    "print(b[0][0]-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c:np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def load_data(path=\"./cora/\", dataset=\"cora\"):\n",
    "    print(\"Loading {} dataset...\".format(dataset))\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset), dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j:i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset), dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
    "    adj = adj + adj.T.multiply(adj.T>adj) - adj.multiply(adj.T>adj) # 最后减的那一项目的是去除负边\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "def normalize(mx):\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv) # 此处得到一个对角矩阵\n",
    "    mx = r_mat_inv.dot(mx) # 注意.dot为矩阵乘法,不是对应元素相乘\n",
    "    return mx\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = sparse_mx.shape\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  3  2]\n",
      " [ 0  4  5]\n",
      " [ 1  3  6]\n",
      " [ 1  5  7]\n",
      " [ 2  4 10]]\n",
      "--------------------------------------------------\n",
      "  (0, 3)\t2.0\n",
      "  (0, 4)\t5.0\n",
      "  (1, 3)\t6.0\n",
      "  (1, 5)\t7.0\n",
      "  (2, 4)\t10.0\n",
      "--------------------------------------------------\n",
      "[[ 0.  0.  0.  2.  5.  0.  0.]\n",
      " [ 0.  0.  0.  6.  0.  7.  0.]\n",
      " [ 0.  0.  0.  0. 10.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# 理解.todense()\n",
    "a = np.array([[0,3,2],[0,4,5],[1,3,6],[1,5,7],[2,4,10]])\n",
    "b = sp.coo_matrix(arg1=(a[:, 2], (a[:, 0], a[:, 1])), shape=(7,7), dtype=np.float32)\n",
    "c = b.todense()\n",
    "print(a)\n",
    "print(\"-\"*50)\n",
    "print(b)\n",
    "print(\"-\"*50)\n",
    "print(c)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5996, 0.1680, 0.2345, 0.3149, 0.4493],\n",
      "        [0.5121, 0.4475, 0.0147, 0.8968, 0.7816],\n",
      "        [0.8191, 0.9424, 0.1708, 0.4721, 0.5223]])\n",
      "torch.return_types.max(\n",
      "values=tensor([0.5996, 0.8968, 0.9424]),\n",
      "indices=tensor([0, 3, 1]))\n",
      "tensor([0, 3, 1], dtype=torch.int32)\n",
      "int32\n"
     ]
    }
   ],
   "source": [
    "# # 理解np.where\n",
    "labels = encode_onehot(idx_features_labels[:, -1])\n",
    "# print(labels)\n",
    "# print(np.where(labels)[0])\n",
    "# print(np.where(labels)[1])\n",
    "# a = [[1, 4, 0, -2, 0, 3, 0, 0, -5],[1, 4, 0, 2, 0, 3, 0, 0, 5]]\n",
    "# print(np.where(a)[0])\n",
    "# print(np.where(a)[1])\n",
    "# print(torch.tensor(a).double())\n",
    "# 理解.max(1)[1], .type_as(), .double()\n",
    "b = torch.rand(3,5)\n",
    "print(b)\n",
    "print(b.max(1))\n",
    "print(b.max(1)[1].type_as(torch.tensor(labels)))\n",
    "print(labels.dtype)\n",
    "# # 理解.astype(np.float32)\n",
    "# c = np.array(b).astype(np.float32) # 只有np.array才能用astype\n",
    "# print(c)\n",
    "# print(b.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 5.]\n",
      " [0. 2. 0.]\n",
      " [0. 0. 3.]]\n",
      "[6. 2. 3.]\n",
      "[0.16666667 0.5        0.33333333]\n",
      "  (0, 0)\t0.16666666666666666\n",
      "  (1, 1)\t0.5\n",
      "  (2, 2)\t0.3333333333333333\n",
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "# 理解函数normalize的细节\n",
    "a = np.array([[1, 0, 5], [0, 2, 0], [0, 0, 3]], dtype=np.float64)\n",
    "print(a)\n",
    "# b = np.power(a, -1).flatten()\n",
    "b = np.array(a.sum(1))\n",
    "print(b)\n",
    "b = np.power(b, -1).flatten()\n",
    "b[np.isinf(b)] = 0.\n",
    "print(b)\n",
    "c = a.flatten()\n",
    "d = sp.diags(b)\n",
    "print(d)\n",
    "print(d.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 163  402]\n",
      " [ 163  659]\n",
      " [ 163 1696]\n",
      " ...\n",
      " [1887 2258]\n",
      " [1902 1887]\n",
      " [ 837 1686]]\n",
      "  (163, 402)\t1.0\n",
      "  (163, 659)\t1.0\n",
      "  (163, 1696)\t1.0\n",
      "  (163, 2295)\t1.0\n",
      "  (163, 1274)\t1.0\n",
      "  (163, 1286)\t1.0\n",
      "  (163, 1544)\t1.0\n",
      "  (163, 2600)\t1.0\n",
      "  (163, 2363)\t1.0\n",
      "  (163, 1905)\t1.0\n",
      "  (163, 1611)\t1.0\n",
      "  (163, 141)\t1.0\n",
      "  (163, 1807)\t1.0\n",
      "  (163, 1110)\t1.0\n",
      "  (163, 174)\t1.0\n",
      "  (163, 2521)\t1.0\n",
      "  (163, 1792)\t1.0\n",
      "  (163, 1675)\t1.0\n",
      "  (163, 1334)\t1.0\n",
      "  (163, 813)\t1.0\n",
      "  (163, 1799)\t1.0\n",
      "  (163, 1943)\t1.0\n",
      "  (163, 2077)\t1.0\n",
      "  (163, 765)\t1.0\n",
      "  (163, 769)\t1.0\n",
      "  :\t:\n",
      "  (2228, 1093)\t1.0\n",
      "  (2228, 1094)\t1.0\n",
      "  (2228, 2068)\t1.0\n",
      "  (2228, 2085)\t1.0\n",
      "  (2694, 2331)\t1.0\n",
      "  (617, 226)\t1.0\n",
      "  (422, 1691)\t1.0\n",
      "  (2142, 2096)\t1.0\n",
      "  (1477, 1252)\t1.0\n",
      "  (1485, 1252)\t1.0\n",
      "  (2185, 2109)\t1.0\n",
      "  (2117, 2639)\t1.0\n",
      "  (1211, 1247)\t1.0\n",
      "  (1884, 745)\t1.0\n",
      "  (1884, 1886)\t1.0\n",
      "  (1884, 1902)\t1.0\n",
      "  (1885, 745)\t1.0\n",
      "  (1885, 1884)\t1.0\n",
      "  (1885, 1886)\t1.0\n",
      "  (1885, 1902)\t1.0\n",
      "  (1886, 745)\t1.0\n",
      "  (1886, 1902)\t1.0\n",
      "  (1887, 2258)\t1.0\n",
      "  (1902, 1887)\t1.0\n",
      "  (837, 1686)\t1.0\n"
     ]
    }
   ],
   "source": [
    "# 理解np.genfromtxt和sp.csr_matrix的处理：\n",
    "labels = encode_onehot(idx_features_labels[:, -1])\n",
    "idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "idx_map = {j: i for i, j in enumerate(idx)}\n",
    "edges_unordered = np.genfromtxt(\"{}{}.cites\".format(\"./cora/\", \"cora\"),dtype=np.int32)\n",
    "# print(edges_unordered.shape)\n",
    "# print(edges_unordered.flatten().shape)\n",
    "edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "# 为了格式能成功转换,此处需要调用.flatten()改变shape,最后再转回来\n",
    "print(edges)\n",
    "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
    "print(adj)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(dropout=0.5, epochs=200, fastmode=True, hidden=16, lr=0.01, no_cuda=False, seed=42, weight_decay=0.0005)\n",
      "Loading cora dataset...\n",
      "Epoch:0001 loss_train:2.0485 acc_train:0.0643 loss_val:2.0637 acc_val:0.0700 time:0.0101\n",
      "Epoch:0002 loss_train:2.0326 acc_train:0.0786 loss_val:2.0459 acc_val:0.0967 time:0.0101\n",
      "Epoch:0003 loss_train:2.0225 acc_train:0.0571 loss_val:2.0379 acc_val:0.0800 time:0.0101\n",
      "Epoch:0004 loss_train:2.0085 acc_train:0.0786 loss_val:2.0222 acc_val:0.1033 time:0.0102\n",
      "Epoch:0005 loss_train:1.9951 acc_train:0.0571 loss_val:2.0053 acc_val:0.0733 time:0.0101\n",
      "Epoch:0006 loss_train:1.9881 acc_train:0.0857 loss_val:2.0061 acc_val:0.0633 time:0.0101\n",
      "Epoch:0007 loss_train:1.9754 acc_train:0.0857 loss_val:1.9850 acc_val:0.1000 time:0.0101\n",
      "Epoch:0008 loss_train:1.9647 acc_train:0.1000 loss_val:1.9734 acc_val:0.0700 time:0.0101\n",
      "Epoch:0009 loss_train:1.9601 acc_train:0.1071 loss_val:1.9650 acc_val:0.1300 time:0.0101\n",
      "Epoch:0010 loss_train:1.9403 acc_train:0.1357 loss_val:1.9507 acc_val:0.1067 time:0.0202\n",
      "Epoch:0011 loss_train:1.9302 acc_train:0.1429 loss_val:1.9351 acc_val:0.1600 time:0.0101\n",
      "Epoch:0012 loss_train:1.9285 acc_train:0.1571 loss_val:1.9277 acc_val:0.2267 time:0.0100\n",
      "Epoch:0013 loss_train:1.9104 acc_train:0.2500 loss_val:1.9148 acc_val:0.2133 time:0.0101\n",
      "Epoch:0014 loss_train:1.8976 acc_train:0.3500 loss_val:1.8984 acc_val:0.3300 time:0.0100\n",
      "Epoch:0015 loss_train:1.8842 acc_train:0.3143 loss_val:1.8891 acc_val:0.3200 time:0.0100\n",
      "Epoch:0016 loss_train:1.8596 acc_train:0.3714 loss_val:1.8828 acc_val:0.3133 time:0.0157\n",
      "Epoch:0017 loss_train:1.8660 acc_train:0.2857 loss_val:1.8612 acc_val:0.3667 time:0.0101\n",
      "Epoch:0018 loss_train:1.8278 acc_train:0.3643 loss_val:1.8298 acc_val:0.3500 time:0.0101\n",
      "Epoch:0019 loss_train:1.8128 acc_train:0.3357 loss_val:1.8308 acc_val:0.3500 time:0.0100\n",
      "Epoch:0020 loss_train:1.8192 acc_train:0.2929 loss_val:1.8271 acc_val:0.3633 time:0.0101\n",
      "Epoch:0021 loss_train:1.8074 acc_train:0.3071 loss_val:1.8185 acc_val:0.3200 time:0.0101\n",
      "Epoch:0022 loss_train:1.7966 acc_train:0.3000 loss_val:1.8084 acc_val:0.3367 time:0.0102\n",
      "Epoch:0023 loss_train:1.7828 acc_train:0.2929 loss_val:1.7902 acc_val:0.3433 time:0.0101\n",
      "Epoch:0024 loss_train:1.7703 acc_train:0.3000 loss_val:1.7618 acc_val:0.3467 time:0.0101\n",
      "Epoch:0025 loss_train:1.7700 acc_train:0.3071 loss_val:1.7908 acc_val:0.3300 time:0.0000\n",
      "Epoch:0026 loss_train:1.7397 acc_train:0.3000 loss_val:1.7668 acc_val:0.3267 time:0.0001\n",
      "Epoch:0027 loss_train:1.7400 acc_train:0.3143 loss_val:1.7339 acc_val:0.3400 time:0.0101\n",
      "Epoch:0028 loss_train:1.7268 acc_train:0.2857 loss_val:1.7296 acc_val:0.3400 time:0.0201\n",
      "Epoch:0029 loss_train:1.7148 acc_train:0.3000 loss_val:1.7666 acc_val:0.3400 time:0.0101\n",
      "Epoch:0030 loss_train:1.7098 acc_train:0.3000 loss_val:1.7133 acc_val:0.3467 time:0.0101\n",
      "Epoch:0031 loss_train:1.6990 acc_train:0.3071 loss_val:1.7251 acc_val:0.3467 time:0.0101\n",
      "Epoch:0032 loss_train:1.7143 acc_train:0.3000 loss_val:1.7188 acc_val:0.3467 time:0.0000\n",
      "Epoch:0033 loss_train:1.7143 acc_train:0.3071 loss_val:1.7226 acc_val:0.3367 time:0.0101\n",
      "Epoch:0034 loss_train:1.6688 acc_train:0.3071 loss_val:1.7092 acc_val:0.3467 time:0.0000\n",
      "Epoch:0035 loss_train:1.6708 acc_train:0.3000 loss_val:1.6993 acc_val:0.3400 time:0.0202\n",
      "Epoch:0036 loss_train:1.6481 acc_train:0.3071 loss_val:1.6605 acc_val:0.3467 time:0.0101\n",
      "Epoch:0037 loss_train:1.6633 acc_train:0.3000 loss_val:1.6894 acc_val:0.3500 time:0.0101\n",
      "Epoch:0038 loss_train:1.6679 acc_train:0.3143 loss_val:1.6934 acc_val:0.3467 time:0.0101\n",
      "Epoch:0039 loss_train:1.6407 acc_train:0.3500 loss_val:1.6811 acc_val:0.3400 time:0.0100\n",
      "Epoch:0040 loss_train:1.5859 acc_train:0.3643 loss_val:1.6402 acc_val:0.3633 time:0.0101\n",
      "Epoch:0041 loss_train:1.6139 acc_train:0.3571 loss_val:1.6786 acc_val:0.3667 time:0.0100\n",
      "Epoch:0042 loss_train:1.5800 acc_train:0.4286 loss_val:1.6228 acc_val:0.3833 time:0.0101\n",
      "Epoch:0043 loss_train:1.5645 acc_train:0.4143 loss_val:1.6560 acc_val:0.3967 time:0.0000\n",
      "Epoch:0044 loss_train:1.5688 acc_train:0.4643 loss_val:1.6102 acc_val:0.4100 time:0.0000\n",
      "Epoch:0045 loss_train:1.5280 acc_train:0.4786 loss_val:1.6208 acc_val:0.4033 time:0.0101\n",
      "Epoch:0046 loss_train:1.5430 acc_train:0.4286 loss_val:1.6316 acc_val:0.4100 time:0.0000\n",
      "Epoch:0047 loss_train:1.5322 acc_train:0.4643 loss_val:1.6003 acc_val:0.4467 time:0.0201\n",
      "Epoch:0048 loss_train:1.4754 acc_train:0.5143 loss_val:1.5891 acc_val:0.4367 time:0.0101\n",
      "Epoch:0049 loss_train:1.4502 acc_train:0.5143 loss_val:1.5589 acc_val:0.4367 time:0.0101\n",
      "Epoch:0050 loss_train:1.4466 acc_train:0.5071 loss_val:1.5670 acc_val:0.4733 time:0.0000\n",
      "Epoch:0051 loss_train:1.4866 acc_train:0.5286 loss_val:1.5824 acc_val:0.4567 time:0.0101\n",
      "Epoch:0052 loss_train:1.4237 acc_train:0.5643 loss_val:1.5340 acc_val:0.4600 time:0.0101\n",
      "Epoch:0053 loss_train:1.4005 acc_train:0.5357 loss_val:1.5365 acc_val:0.4667 time:0.0101\n",
      "Epoch:0054 loss_train:1.4162 acc_train:0.6000 loss_val:1.5220 acc_val:0.5067 time:0.0100\n",
      "Epoch:0055 loss_train:1.4091 acc_train:0.5286 loss_val:1.5041 acc_val:0.4833 time:0.0101\n",
      "Epoch:0056 loss_train:1.4048 acc_train:0.5643 loss_val:1.5101 acc_val:0.4800 time:0.0100\n",
      "Epoch:0057 loss_train:1.3803 acc_train:0.6143 loss_val:1.4808 acc_val:0.5333 time:0.0000\n",
      "Epoch:0058 loss_train:1.3325 acc_train:0.6286 loss_val:1.4713 acc_val:0.5200 time:0.0101\n",
      "Epoch:0059 loss_train:1.3523 acc_train:0.5857 loss_val:1.4956 acc_val:0.5300 time:0.0100\n",
      "Epoch:0060 loss_train:1.2909 acc_train:0.6214 loss_val:1.4595 acc_val:0.5233 time:0.0101\n",
      "Epoch:0061 loss_train:1.3166 acc_train:0.5929 loss_val:1.4758 acc_val:0.4700 time:0.0100\n",
      "Epoch:0062 loss_train:1.2945 acc_train:0.6071 loss_val:1.4432 acc_val:0.5233 time:0.0101\n",
      "Epoch:0063 loss_train:1.2500 acc_train:0.6643 loss_val:1.4069 acc_val:0.5633 time:0.0100\n",
      "Epoch:0064 loss_train:1.2727 acc_train:0.6429 loss_val:1.4466 acc_val:0.5567 time:0.0100\n",
      "Epoch:0065 loss_train:1.2193 acc_train:0.6786 loss_val:1.4069 acc_val:0.5967 time:0.0157\n",
      "Epoch:0066 loss_train:1.2377 acc_train:0.6643 loss_val:1.3874 acc_val:0.6067 time:0.0100\n",
      "Epoch:0067 loss_train:1.1659 acc_train:0.7214 loss_val:1.3472 acc_val:0.6033 time:0.0101\n",
      "Epoch:0068 loss_train:1.1784 acc_train:0.7000 loss_val:1.3622 acc_val:0.6233 time:0.0000\n",
      "Epoch:0069 loss_train:1.1729 acc_train:0.6929 loss_val:1.3534 acc_val:0.6267 time:0.0100\n",
      "Epoch:0070 loss_train:1.1801 acc_train:0.7071 loss_val:1.3485 acc_val:0.6367 time:0.0101\n",
      "Epoch:0071 loss_train:1.1628 acc_train:0.7000 loss_val:1.3028 acc_val:0.6133 time:0.0100\n",
      "Epoch:0072 loss_train:1.0967 acc_train:0.7286 loss_val:1.3162 acc_val:0.6400 time:0.0101\n",
      "Epoch:0073 loss_train:1.1558 acc_train:0.6929 loss_val:1.3483 acc_val:0.6200 time:0.0100\n",
      "Epoch:0074 loss_train:1.1297 acc_train:0.7214 loss_val:1.3141 acc_val:0.6133 time:0.0101\n",
      "Epoch:0075 loss_train:1.0859 acc_train:0.7286 loss_val:1.3047 acc_val:0.6100 time:0.0101\n",
      "Epoch:0076 loss_train:1.0726 acc_train:0.7214 loss_val:1.2866 acc_val:0.6067 time:0.0101\n",
      "Epoch:0077 loss_train:1.1079 acc_train:0.7429 loss_val:1.3104 acc_val:0.6267 time:0.0101\n",
      "Epoch:0078 loss_train:1.0457 acc_train:0.7214 loss_val:1.2757 acc_val:0.6633 time:0.0101\n",
      "Epoch:0079 loss_train:1.0421 acc_train:0.7643 loss_val:1.2604 acc_val:0.6233 time:0.0101\n",
      "Epoch:0080 loss_train:1.0433 acc_train:0.7500 loss_val:1.2309 acc_val:0.6567 time:0.0101\n",
      "Epoch:0081 loss_train:0.9936 acc_train:0.7714 loss_val:1.2267 acc_val:0.6533 time:0.0101\n",
      "Epoch:0082 loss_train:0.9817 acc_train:0.7857 loss_val:1.2080 acc_val:0.6633 time:0.0100\n",
      "Epoch:0083 loss_train:1.0161 acc_train:0.7429 loss_val:1.2182 acc_val:0.6567 time:0.0101\n",
      "Epoch:0084 loss_train:0.9702 acc_train:0.7500 loss_val:1.2210 acc_val:0.6500 time:0.0100\n",
      "Epoch:0085 loss_train:0.9249 acc_train:0.7643 loss_val:1.1549 acc_val:0.6533 time:0.0101\n",
      "Epoch:0086 loss_train:0.9500 acc_train:0.7786 loss_val:1.1857 acc_val:0.6467 time:0.0101\n",
      "Epoch:0087 loss_train:0.9443 acc_train:0.7500 loss_val:1.1713 acc_val:0.6767 time:0.0101\n",
      "Epoch:0088 loss_train:0.9862 acc_train:0.7643 loss_val:1.1843 acc_val:0.6700 time:0.0101\n",
      "Epoch:0089 loss_train:0.9553 acc_train:0.7500 loss_val:1.1513 acc_val:0.6767 time:0.0100\n",
      "Epoch:0090 loss_train:0.9378 acc_train:0.7571 loss_val:1.2109 acc_val:0.6400 time:0.0157\n",
      "Epoch:0091 loss_train:0.9481 acc_train:0.7786 loss_val:1.1398 acc_val:0.6933 time:0.0100\n",
      "Epoch:0092 loss_train:0.9316 acc_train:0.7786 loss_val:1.1742 acc_val:0.6667 time:0.0101\n",
      "Epoch:0093 loss_train:0.8998 acc_train:0.7786 loss_val:1.1690 acc_val:0.6467 time:0.0000\n",
      "Epoch:0094 loss_train:0.8668 acc_train:0.7929 loss_val:1.1056 acc_val:0.7033 time:0.0100\n",
      "Epoch:0095 loss_train:0.8721 acc_train:0.7714 loss_val:1.1228 acc_val:0.6767 time:0.0101\n",
      "Epoch:0096 loss_train:0.8591 acc_train:0.7714 loss_val:1.1460 acc_val:0.6667 time:0.0101\n",
      "Epoch:0097 loss_train:0.8591 acc_train:0.7786 loss_val:1.0978 acc_val:0.6800 time:0.0101\n",
      "Epoch:0098 loss_train:0.8756 acc_train:0.7929 loss_val:1.1129 acc_val:0.6933 time:0.0100\n",
      "Epoch:0099 loss_train:0.8758 acc_train:0.8214 loss_val:1.0885 acc_val:0.7200 time:0.0101\n",
      "Epoch:0100 loss_train:0.8414 acc_train:0.7929 loss_val:1.0857 acc_val:0.6833 time:0.0100\n",
      "Epoch:0101 loss_train:0.8167 acc_train:0.8071 loss_val:1.1100 acc_val:0.7133 time:0.0101\n",
      "Epoch:0102 loss_train:0.8049 acc_train:0.7929 loss_val:1.0768 acc_val:0.7033 time:0.0100\n",
      "Epoch:0103 loss_train:0.8230 acc_train:0.7857 loss_val:1.1190 acc_val:0.6867 time:0.0101\n",
      "Epoch:0104 loss_train:0.7969 acc_train:0.8143 loss_val:1.0549 acc_val:0.7100 time:0.0100\n",
      "Epoch:0105 loss_train:0.7884 acc_train:0.8429 loss_val:1.0725 acc_val:0.6933 time:0.0100\n",
      "Epoch:0106 loss_train:0.8073 acc_train:0.7857 loss_val:1.0376 acc_val:0.7067 time:0.0000\n",
      "Epoch:0107 loss_train:0.7991 acc_train:0.7929 loss_val:1.0879 acc_val:0.6933 time:0.0157\n",
      "Epoch:0108 loss_train:0.8138 acc_train:0.8000 loss_val:1.0618 acc_val:0.7033 time:0.0100\n",
      "Epoch:0109 loss_train:0.7619 acc_train:0.8500 loss_val:1.0754 acc_val:0.6933 time:0.0101\n",
      "Epoch:0110 loss_train:0.8043 acc_train:0.8286 loss_val:1.0587 acc_val:0.6967 time:0.0101\n",
      "Epoch:0111 loss_train:0.7959 acc_train:0.8357 loss_val:1.0630 acc_val:0.6933 time:0.0101\n",
      "Epoch:0112 loss_train:0.7621 acc_train:0.8214 loss_val:1.0808 acc_val:0.6833 time:0.0100\n",
      "Epoch:0113 loss_train:0.7781 acc_train:0.8357 loss_val:1.0248 acc_val:0.7167 time:0.0101\n",
      "Epoch:0114 loss_train:0.7907 acc_train:0.8000 loss_val:1.0505 acc_val:0.7167 time:0.0101\n",
      "Epoch:0115 loss_train:0.7760 acc_train:0.8286 loss_val:1.0171 acc_val:0.7267 time:0.0169\n",
      "Epoch:0116 loss_train:0.7437 acc_train:0.8286 loss_val:1.0001 acc_val:0.7333 time:0.0046\n",
      "Epoch:0117 loss_train:0.7388 acc_train:0.8357 loss_val:1.0401 acc_val:0.7200 time:0.0100\n",
      "Epoch:0118 loss_train:0.7667 acc_train:0.8286 loss_val:1.0434 acc_val:0.7033 time:0.0101\n",
      "Epoch:0119 loss_train:0.6850 acc_train:0.8500 loss_val:0.9702 acc_val:0.7267 time:0.0000\n",
      "Epoch:0120 loss_train:0.7258 acc_train:0.8714 loss_val:1.0111 acc_val:0.7067 time:0.0100\n",
      "Epoch:0121 loss_train:0.6598 acc_train:0.8357 loss_val:0.9852 acc_val:0.7100 time:0.0101\n",
      "Epoch:0122 loss_train:0.6902 acc_train:0.8714 loss_val:1.0014 acc_val:0.7500 time:0.0100\n",
      "Epoch:0123 loss_train:0.6619 acc_train:0.8929 loss_val:1.0301 acc_val:0.7167 time:0.0101\n",
      "Epoch:0124 loss_train:0.7005 acc_train:0.8643 loss_val:0.9966 acc_val:0.7300 time:0.0101\n",
      "Epoch:0125 loss_train:0.6814 acc_train:0.8643 loss_val:0.9657 acc_val:0.7600 time:0.0101\n",
      "Epoch:0126 loss_train:0.6962 acc_train:0.8929 loss_val:0.9926 acc_val:0.7233 time:0.0100\n",
      "Epoch:0127 loss_train:0.7209 acc_train:0.8500 loss_val:1.0289 acc_val:0.7167 time:0.0101\n",
      "Epoch:0128 loss_train:0.6806 acc_train:0.8429 loss_val:0.9700 acc_val:0.7367 time:0.0000\n",
      "Epoch:0129 loss_train:0.6826 acc_train:0.8571 loss_val:0.9902 acc_val:0.7233 time:0.0101\n",
      "Epoch:0130 loss_train:0.6985 acc_train:0.8429 loss_val:0.9987 acc_val:0.7200 time:0.0101\n",
      "Epoch:0131 loss_train:0.7065 acc_train:0.8786 loss_val:1.0302 acc_val:0.6933 time:0.0100\n",
      "Epoch:0132 loss_train:0.6978 acc_train:0.8500 loss_val:1.0240 acc_val:0.7333 time:0.0102\n",
      "Epoch:0133 loss_train:0.6270 acc_train:0.8929 loss_val:0.9272 acc_val:0.7467 time:0.0101\n",
      "Epoch:0134 loss_train:0.6765 acc_train:0.8714 loss_val:0.9827 acc_val:0.7500 time:0.0101\n",
      "Epoch:0135 loss_train:0.6478 acc_train:0.8714 loss_val:0.9597 acc_val:0.7533 time:0.0101\n",
      "Epoch:0136 loss_train:0.6547 acc_train:0.8643 loss_val:1.0093 acc_val:0.6967 time:0.0101\n",
      "Epoch:0137 loss_train:0.6320 acc_train:0.8786 loss_val:0.9588 acc_val:0.7667 time:0.0101\n",
      "Epoch:0138 loss_train:0.6116 acc_train:0.8714 loss_val:0.9659 acc_val:0.7367 time:0.0101\n",
      "Epoch:0139 loss_train:0.6659 acc_train:0.8714 loss_val:0.9051 acc_val:0.7500 time:0.0101\n",
      "Epoch:0140 loss_train:0.6064 acc_train:0.8714 loss_val:0.9383 acc_val:0.7400 time:0.0100\n",
      "Epoch:0141 loss_train:0.6302 acc_train:0.8929 loss_val:0.9348 acc_val:0.7700 time:0.0101\n",
      "Epoch:0142 loss_train:0.6837 acc_train:0.8357 loss_val:0.9592 acc_val:0.7467 time:0.0101\n",
      "Epoch:0143 loss_train:0.6095 acc_train:0.8786 loss_val:0.9733 acc_val:0.7333 time:0.0101\n",
      "Epoch:0144 loss_train:0.6265 acc_train:0.8714 loss_val:0.9706 acc_val:0.7267 time:0.0101\n",
      "Epoch:0145 loss_train:0.5949 acc_train:0.9071 loss_val:0.9107 acc_val:0.7467 time:0.0101\n",
      "Epoch:0146 loss_train:0.5926 acc_train:0.8929 loss_val:0.8986 acc_val:0.7600 time:0.0101\n",
      "Epoch:0147 loss_train:0.5839 acc_train:0.8929 loss_val:0.9219 acc_val:0.7700 time:0.0101\n",
      "Epoch:0148 loss_train:0.6337 acc_train:0.8643 loss_val:0.9211 acc_val:0.7667 time:0.0101\n",
      "Epoch:0149 loss_train:0.5976 acc_train:0.8857 loss_val:0.8568 acc_val:0.7933 time:0.0102\n",
      "Epoch:0150 loss_train:0.6197 acc_train:0.8714 loss_val:0.9822 acc_val:0.7467 time:0.0101\n",
      "Epoch:0151 loss_train:0.6158 acc_train:0.8929 loss_val:0.9533 acc_val:0.7400 time:0.0101\n",
      "Epoch:0152 loss_train:0.5740 acc_train:0.8857 loss_val:0.8867 acc_val:0.7733 time:0.0100\n",
      "Epoch:0153 loss_train:0.5414 acc_train:0.8929 loss_val:0.9472 acc_val:0.7333 time:0.0102\n",
      "Epoch:0154 loss_train:0.6024 acc_train:0.8786 loss_val:0.9524 acc_val:0.7400 time:0.0100\n",
      "Epoch:0155 loss_train:0.5535 acc_train:0.8929 loss_val:0.8970 acc_val:0.7633 time:0.0102\n",
      "Epoch:0156 loss_train:0.5471 acc_train:0.9000 loss_val:0.8605 acc_val:0.7567 time:0.0101\n",
      "Epoch:0157 loss_train:0.5431 acc_train:0.9143 loss_val:0.8978 acc_val:0.7633 time:0.0101\n",
      "Epoch:0158 loss_train:0.5741 acc_train:0.8857 loss_val:0.9093 acc_val:0.7700 time:0.0101\n",
      "Epoch:0159 loss_train:0.5896 acc_train:0.8857 loss_val:0.8682 acc_val:0.7667 time:0.0101\n",
      "Epoch:0160 loss_train:0.5701 acc_train:0.9071 loss_val:0.8681 acc_val:0.7600 time:0.0103\n",
      "Epoch:0161 loss_train:0.5951 acc_train:0.8857 loss_val:0.9618 acc_val:0.7367 time:0.0100\n",
      "Epoch:0162 loss_train:0.5258 acc_train:0.9214 loss_val:0.8795 acc_val:0.7700 time:0.0100\n",
      "Epoch:0163 loss_train:0.5797 acc_train:0.8929 loss_val:0.9102 acc_val:0.7367 time:0.0157\n",
      "Epoch:0164 loss_train:0.5951 acc_train:0.8786 loss_val:0.9028 acc_val:0.7667 time:0.0101\n",
      "Epoch:0165 loss_train:0.5505 acc_train:0.8857 loss_val:0.9019 acc_val:0.7533 time:0.0102\n",
      "Epoch:0166 loss_train:0.5587 acc_train:0.8929 loss_val:0.8951 acc_val:0.7500 time:0.0100\n",
      "Epoch:0167 loss_train:0.5749 acc_train:0.8786 loss_val:0.8862 acc_val:0.7533 time:0.0101\n",
      "Epoch:0168 loss_train:0.5477 acc_train:0.9071 loss_val:0.9094 acc_val:0.7667 time:0.0100\n",
      "Epoch:0169 loss_train:0.5648 acc_train:0.8714 loss_val:0.8969 acc_val:0.7433 time:0.0101\n",
      "Epoch:0170 loss_train:0.5585 acc_train:0.8857 loss_val:0.8447 acc_val:0.7667 time:0.0101\n",
      "Epoch:0171 loss_train:0.5293 acc_train:0.8857 loss_val:0.8390 acc_val:0.7467 time:0.0101\n",
      "Epoch:0172 loss_train:0.5391 acc_train:0.9071 loss_val:0.8696 acc_val:0.7633 time:0.0100\n",
      "Epoch:0173 loss_train:0.4909 acc_train:0.9071 loss_val:0.9120 acc_val:0.7533 time:0.0101\n",
      "Epoch:0174 loss_train:0.5503 acc_train:0.8857 loss_val:0.8856 acc_val:0.7767 time:0.0100\n",
      "Epoch:0175 loss_train:0.5292 acc_train:0.9000 loss_val:0.9144 acc_val:0.7533 time:0.0101\n",
      "Epoch:0176 loss_train:0.5227 acc_train:0.8857 loss_val:0.8507 acc_val:0.7633 time:0.0101\n",
      "Epoch:0177 loss_train:0.5320 acc_train:0.9000 loss_val:0.8429 acc_val:0.7600 time:0.0102\n",
      "Epoch:0178 loss_train:0.5324 acc_train:0.8857 loss_val:0.8819 acc_val:0.7567 time:0.0201\n",
      "Epoch:0179 loss_train:0.5025 acc_train:0.9214 loss_val:0.8889 acc_val:0.7533 time:0.0100\n",
      "Epoch:0180 loss_train:0.5183 acc_train:0.9000 loss_val:0.8460 acc_val:0.7600 time:0.0101\n",
      "Epoch:0181 loss_train:0.5220 acc_train:0.9143 loss_val:0.8743 acc_val:0.7600 time:0.0100\n",
      "Epoch:0182 loss_train:0.5132 acc_train:0.9143 loss_val:0.8664 acc_val:0.7667 time:0.0101\n",
      "Epoch:0183 loss_train:0.4776 acc_train:0.9143 loss_val:0.8143 acc_val:0.7900 time:0.0100\n",
      "Epoch:0184 loss_train:0.4409 acc_train:0.9214 loss_val:0.8766 acc_val:0.7867 time:0.0101\n",
      "Epoch:0185 loss_train:0.5077 acc_train:0.9071 loss_val:0.8418 acc_val:0.7900 time:0.0101\n",
      "Epoch:0186 loss_train:0.4789 acc_train:0.9143 loss_val:0.8223 acc_val:0.7800 time:0.0101\n",
      "Epoch:0187 loss_train:0.4729 acc_train:0.9357 loss_val:0.8390 acc_val:0.7667 time:0.0101\n",
      "Epoch:0188 loss_train:0.4852 acc_train:0.9000 loss_val:0.8516 acc_val:0.7767 time:0.0101\n",
      "Epoch:0189 loss_train:0.4399 acc_train:0.9357 loss_val:0.8854 acc_val:0.7467 time:0.0101\n",
      "Epoch:0190 loss_train:0.4897 acc_train:0.9071 loss_val:0.8207 acc_val:0.7667 time:0.0101\n",
      "Epoch:0191 loss_train:0.4970 acc_train:0.8857 loss_val:0.8467 acc_val:0.7800 time:0.0101\n",
      "Epoch:0192 loss_train:0.4914 acc_train:0.8929 loss_val:0.8330 acc_val:0.7700 time:0.0101\n",
      "Epoch:0193 loss_train:0.4686 acc_train:0.9143 loss_val:0.7983 acc_val:0.7967 time:0.0100\n",
      "Epoch:0194 loss_train:0.5031 acc_train:0.9286 loss_val:0.9007 acc_val:0.7400 time:0.0101\n",
      "Epoch:0195 loss_train:0.4975 acc_train:0.9000 loss_val:0.8023 acc_val:0.7833 time:0.0101\n",
      "Epoch:0196 loss_train:0.5017 acc_train:0.9071 loss_val:0.8449 acc_val:0.7533 time:0.0101\n",
      "Epoch:0197 loss_train:0.4716 acc_train:0.9286 loss_val:0.8527 acc_val:0.7533 time:0.0100\n",
      "Epoch:0198 loss_train:0.4892 acc_train:0.9143 loss_val:0.8369 acc_val:0.7833 time:0.0101\n",
      "Epoch:0199 loss_train:0.4613 acc_train:0.9286 loss_val:0.8729 acc_val:0.7567 time:0.0101\n",
      "Epoch:0200 loss_train:0.4302 acc_train:0.9286 loss_val:0.8507 acc_val:0.7633 time:0.0101\n",
      "Total time:2.0627s\n",
      "Test set results: loss=0.7772 accuracy=0.8170\n"
     ]
    }
   ],
   "source": [
    "# from __future__ import division\n",
    "# from __future__ import print_function\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--no-cuda\", action=\"store_true\", default=False, help=\"Disables CUDA training.\")\n",
    "parser.add_argument(\"--fastmode\", action=\"store_true\", default=True, help=\"Validate during training pass.\")\n",
    "parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed.\")\n",
    "parser.add_argument(\"--epochs\", type=int, default=200, help=\"Number of epochs to train.\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.01, help=\"Initial learning rate.\")\n",
    "parser.add_argument(\"--weight_decay\", type=float, default=5e-4, help=\"Weight decay(L2 loss on parameters).\")\n",
    "parser.add_argument(\"--hidden\", type=int, default=16, help=\"Number of hidden units\")\n",
    "parser.add_argument(\"--dropout\", type=float, default=0.5, help=\"Dropout rate (1 - keep probability).\")\n",
    "# args = parser.parse_args()\n",
    "args =parser.parse_known_args()[0]\n",
    "# print(parser)\n",
    "print(args)\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "np.random.seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
    "model = GCN(nfeat=features.shape[1], nhid=args.hidden, nclass=labels.max().item()+1, dropout=args.dropout)\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not args.fastmode:\n",
    "        model.eval() # 注意：dropout会影响前向传播,从而影响预测结果\n",
    "        output = model(features, adj)\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print(\"Epoch:{:04d}\".format(epoch+1), \"loss_train:{:.4f}\".format(loss_train.item()), \"acc_train:{:.4f}\".format(acc_train.item()), \"loss_val:{:.4f}\".format(loss_val.item()), \"acc_val:{:.4f}\".format(acc_val.item()), \"time:{:.4f}\".format(time.time()-t))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\", \"loss={:.4f}\".format(loss_test.item()), \"accuracy={:.4f}\".format(acc_test.item()))\n",
    "\n",
    "t_total = time.time()\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch)\n",
    "print(\"Total time:{:.4f}s\".format(time.time()-t_total))\n",
    "\n",
    "test()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}