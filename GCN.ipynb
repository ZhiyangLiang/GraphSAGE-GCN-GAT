{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter # 也可直接调用nn.Parameter\n",
    "from torch.nn.modules.module import Module # 也可直接调用nn.Module\n",
    "\n",
    "# 注意：torch.FloatTensor生成的元素数值非常接近0;torch.Long生成的元素数值非常大\n",
    "class GraphConvolution(Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        # self.weight2 = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None) #\n",
    "        self.reset_parameters() # 此处表示生成变量后(即上面的语句运行后),将会进行变量初始化(即执行该语句)\n",
    "\n",
    "    def reset_parameters(self): # 经测试,重写可覆盖\n",
    "        stdv = 1/math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        # self.weight2.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        # support = torch.mm(input, self.weight)    # 尝试：此行与下一行交换顺序\n",
    "        # output = torch.spmm(adj, support) # torch.spmm支持sparse在前,dense在后的矩阵乘法\n",
    "        support = torch.spmm(adj, input)\n",
    "        output = torch.mm(support, self.weight)\n",
    "        if self.bias is not None:         # adj是稀疏矩阵\n",
    "            return output +self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__+\"(\"+str(self.in_features)+\"->\"+str(self.out_features)+\")\"\n",
    "\n",
    "# class Readout(Module):\n",
    "#     def __init__(self, in_features):\n",
    "#         super(Readout, self).__init__()\n",
    "#         self.in_features = in_features\n",
    "#         self.weight = Parameter(torch.FloatTensor(in_features, in_features))\n",
    "#         self.reset_parameters()\n",
    "#\n",
    "#     def reset_parameters(self):\n",
    "#         stdv = 1/math.sqrt(self.weight.size(1))\n",
    "#         self.weight.data.uniform_(-stdv, stdv)\n",
    "#\n",
    "#     def forward(self, input):\n",
    "#         input = torch.sum(input, 0) # 此处不确定\n",
    "#         return torch.mm(input, self.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        # self.gc2 = GraphConvolution(nhid, nhid)\n",
    "        self.dropout = dropout\n",
    "        # self.rd = Readout(nhid)\n",
    "        # self.fc1 = nn.Linear(nhid, 1)\n",
    "        # nn.init.kaiming_normal_(self.fc1.weight)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "        # x = F.relu(self.gc2(x, adj))\n",
    "        # x = F.dropout(x, self.dropout, training=self.training)\n",
    "        # x = F.relu(self.rd(x))\n",
    "        # x = self.fc(x)\n",
    "        # return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{9, 3, 6}\n",
      "[3, 6, 9]\n",
      "{9: array([1., 0., 0.]), 3: array([0., 1., 0.]), 6: array([0., 0., 1.])}\n",
      "[[0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# 理解独热编码的过程：\n",
    "import numpy as np\n",
    "a = [3, 9, 6]\n",
    "b = set(a)\n",
    "print(b)\n",
    "e = sorted(list(set(a)))\n",
    "print(e)\n",
    "c = {t: np.identity(len(b))[i, :] for i, t in enumerate(b)}\n",
    "# c = {t: np.identity(len(b))[i, :] for i, t in enumerate(e)}\n",
    "print(c)\n",
    "d = np.array(list(map(c.get, a)), dtype=np.int32) # c.get为字典内置函数\n",
    "print(d)\n",
    "# print(map(c.get, a))\n",
    "# print(list(map(c.get, a)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "  (0, 118)\t1.0\n",
      "  (0, 125)\t1.0\n",
      "  (0, 176)\t1.0\n",
      "  (0, 252)\t1.0\n",
      "  (0, 351)\t1.0\n",
      "  (0, 456)\t1.0\n",
      "  (0, 507)\t1.0\n",
      "  (0, 521)\t1.0\n",
      "  (0, 619)\t1.0\n",
      "  (0, 648)\t1.0\n",
      "  (0, 698)\t1.0\n",
      "  (0, 702)\t1.0\n",
      "  (0, 734)\t1.0\n",
      "  (0, 845)\t1.0\n",
      "  (0, 902)\t1.0\n",
      "  (0, 1205)\t1.0\n",
      "  (0, 1209)\t1.0\n",
      "  (0, 1236)\t1.0\n",
      "  (0, 1352)\t1.0\n",
      "  (0, 1426)\t1.0\n",
      "  (1, 12)\t1.0\n",
      "  (1, 509)\t1.0\n",
      "  (1, 620)\t1.0\n",
      "  (1, 763)\t1.0\n",
      "  (1, 882)\t1.0\n",
      "  :\t:\n",
      "  (2706, 1253)\t1.0\n",
      "  (2706, 1266)\t1.0\n",
      "  (2706, 1314)\t1.0\n",
      "  (2706, 1350)\t1.0\n",
      "  (2706, 1383)\t1.0\n",
      "  (2706, 1423)\t1.0\n",
      "  (2707, 19)\t1.0\n",
      "  (2707, 67)\t1.0\n",
      "  (2707, 136)\t1.0\n",
      "  (2707, 304)\t1.0\n",
      "  (2707, 422)\t1.0\n",
      "  (2707, 564)\t1.0\n",
      "  (2707, 737)\t1.0\n",
      "  (2707, 774)\t1.0\n",
      "  (2707, 877)\t1.0\n",
      "  (2707, 1073)\t1.0\n",
      "  (2707, 1075)\t1.0\n",
      "  (2707, 1156)\t1.0\n",
      "  (2707, 1178)\t1.0\n",
      "  (2707, 1203)\t1.0\n",
      "  (2707, 1205)\t1.0\n",
      "  (2707, 1301)\t1.0\n",
      "  (2707, 1333)\t1.0\n",
      "  (2707, 1335)\t1.0\n",
      "  (2707, 1351)\t1.0\n",
      "(2708, 1433)\n"
     ]
    }
   ],
   "source": [
    "# 理解np.genfromtxt和sp.csr_matrix的处理：\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "idx_features_labels = np.genfromtxt(\"{}{}.content\".format(\"./cora/\", \"cora\"), dtype=np.dtype(str))\n",
    "# print(idx_features_labels)\n",
    "mid_idx_features_labels = idx_features_labels[:, 1:-1]\n",
    "after_idx_features_labels = np.zeros(mid_idx_features_labels.shape)\n",
    "for i in range(mid_idx_features_labels.shape[0]):\n",
    "    for j in range(mid_idx_features_labels.shape[1]):\n",
    "        after_idx_features_labels[i][j] = int(mid_idx_features_labels[i][j])\n",
    "features = sp.csr_matrix(after_idx_features_labels, dtype=np.float32)\n",
    "print(after_idx_features_labels)\n",
    "print(features)\n",
    "print(features.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "# 注意此处a中的元素为字符串，且直接在a上修改覆盖无法生效，故要创建另一个与a的shape一致的变量\n",
    "a = idx_features_labels[1:5, 1:5]\n",
    "b = np.zeros(a.shape)\n",
    "for i in range(a.shape[0]):\n",
    "    for j in range(a.shape[1]):\n",
    "        # a[i][j] = int(a[i][j])\n",
    "        b[i][j] = int(a[i][j])\n",
    "\n",
    "# print(a[0][0]-1)\n",
    "print(b[0][0]-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c:np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def load_data(path=\"./cora/\", dataset=\"cora\"):\n",
    "    print(\"Loading {} dataset...\".format(dataset))\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset), dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j:i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset), dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
    "    adj = adj + adj.T.multiply(adj.T>adj) - adj.multiply(adj.T>adj) # 最后减的那一项目的是去除负边\n",
    "\n",
    "    features = normalize(features)\n",
    "    # adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "    adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "def normalize(mx):\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv) # 此处得到一个对角矩阵\n",
    "    mx = r_mat_inv.dot(mx) # 注意.dot为矩阵乘法,不是对应元素相乘\n",
    "    return mx\n",
    "\n",
    "def normalize_adj(mx):\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "    # return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
    "    mid = np.dot(r_mat_inv_sqrt, mx)\n",
    "    return np.dot(mid, r_mat_inv_sqrt)\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels).reshape(labels.shape)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = sparse_mx.shape\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  3  2]\n",
      " [ 0  4  5]\n",
      " [ 1  3  6]\n",
      " [ 1  5  7]\n",
      " [ 2  4 10]]\n",
      "--------------------------------------------------\n",
      "  (0, 3)\t2.0\n",
      "  (0, 4)\t5.0\n",
      "  (1, 3)\t6.0\n",
      "  (1, 5)\t7.0\n",
      "  (2, 4)\t10.0\n",
      "--------------------------------------------------\n",
      "[[ 0.  0.  0.  2.  5.  0.  0.]\n",
      " [ 0.  0.  0.  6.  0.  7.  0.]\n",
      " [ 0.  0.  0.  0. 10.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# 理解.todense()\n",
    "a = np.array([[0,3,2],[0,4,5],[1,3,6],[1,5,7],[2,4,10]])\n",
    "b = sp.coo_matrix(arg1=(a[:, 2], (a[:, 0], a[:, 1])), shape=(7,7), dtype=np.float32)\n",
    "c = b.todense()\n",
    "print(a)\n",
    "print(\"-\"*50)\n",
    "print(b)\n",
    "print(\"-\"*50)\n",
    "print(c)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# 理解np.where\n",
    "# labels = encode_onehot(idx_features_labels[:, -1])\n",
    "# print(labels)\n",
    "# print(np.where(labels)[0])\n",
    "# print(np.where(labels)[1])\n",
    "# a = [[1, 4, 0, -2, 0, 3, 0, 0, -5],[1, 4, 0, 2, 0, 3, 0, 0, 5]]\n",
    "# print(np.where(a)[0])\n",
    "# print(np.where(a)[1])\n",
    "# print(torch.tensor(a).double())\n",
    "# 理解.max(1)[1], .type_as(), .double()\n",
    "# b = torch.rand(3,5)\n",
    "# print(b)\n",
    "# print(b.max(1))\n",
    "# print(b.max(1)[1].type_as(torch.tensor(labels)))\n",
    "# print(labels.dtype)\n",
    "# # 理解.astype(np.float32)\n",
    "# c = np.array(b).astype(np.float32) # 只有np.array才能用astype\n",
    "# print(c)\n",
    "# print(b.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 5.]\n",
      " [0. 2. 0.]\n",
      " [0. 0. 3.]]\n",
      "[6. 2. 3.]\n",
      "[0.16666667 0.5        0.33333333]\n",
      "  (0, 0)\t0.16666666666666666\n",
      "  (1, 1)\t0.5\n",
      "  (2, 2)\t0.3333333333333333\n",
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "# 理解函数normalize的细节\n",
    "a = np.array([[1, 0, 5], [0, 2, 0], [0, 0, 3]], dtype=np.float64)\n",
    "print(a)\n",
    "# b = np.power(a, -1).flatten()\n",
    "b = np.array(a.sum(1))\n",
    "print(b)\n",
    "b = np.power(b, -1).flatten()\n",
    "b[np.isinf(b)] = 0.\n",
    "print(b)\n",
    "c = a.flatten()\n",
    "d = sp.diags(b)\n",
    "print(d)\n",
    "print(d.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     35    1033]\n",
      " [     35  103482]\n",
      " [     35  103515]\n",
      " ...\n",
      " [ 853118 1140289]\n",
      " [ 853155  853118]\n",
      " [ 954315 1155073]]\n",
      "--------------------------------------------------\n",
      "[[ 163  402]\n",
      " [ 163  659]\n",
      " [ 163 1696]\n",
      " ...\n",
      " [1887 2258]\n",
      " [1902 1887]\n",
      " [ 837 1686]]\n",
      "  (163, 402)\t1.0\n",
      "  (163, 659)\t1.0\n",
      "  (163, 1696)\t1.0\n",
      "  (163, 2295)\t1.0\n",
      "  (163, 1274)\t1.0\n",
      "  (163, 1286)\t1.0\n",
      "  (163, 1544)\t1.0\n",
      "  (163, 2600)\t1.0\n",
      "  (163, 2363)\t1.0\n",
      "  (163, 1905)\t1.0\n",
      "  (163, 1611)\t1.0\n",
      "  (163, 141)\t1.0\n",
      "  (163, 1807)\t1.0\n",
      "  (163, 1110)\t1.0\n",
      "  (163, 174)\t1.0\n",
      "  (163, 2521)\t1.0\n",
      "  (163, 1792)\t1.0\n",
      "  (163, 1675)\t1.0\n",
      "  (163, 1334)\t1.0\n",
      "  (163, 813)\t1.0\n",
      "  (163, 1799)\t1.0\n",
      "  (163, 1943)\t1.0\n",
      "  (163, 2077)\t1.0\n",
      "  (163, 765)\t1.0\n",
      "  (163, 769)\t1.0\n",
      "  :\t:\n",
      "  (2228, 1093)\t1.0\n",
      "  (2228, 1094)\t1.0\n",
      "  (2228, 2068)\t1.0\n",
      "  (2228, 2085)\t1.0\n",
      "  (2694, 2331)\t1.0\n",
      "  (617, 226)\t1.0\n",
      "  (422, 1691)\t1.0\n",
      "  (2142, 2096)\t1.0\n",
      "  (1477, 1252)\t1.0\n",
      "  (1485, 1252)\t1.0\n",
      "  (2185, 2109)\t1.0\n",
      "  (2117, 2639)\t1.0\n",
      "  (1211, 1247)\t1.0\n",
      "  (1884, 745)\t1.0\n",
      "  (1884, 1886)\t1.0\n",
      "  (1884, 1902)\t1.0\n",
      "  (1885, 745)\t1.0\n",
      "  (1885, 1884)\t1.0\n",
      "  (1885, 1886)\t1.0\n",
      "  (1885, 1902)\t1.0\n",
      "  (1886, 745)\t1.0\n",
      "  (1886, 1902)\t1.0\n",
      "  (1887, 2258)\t1.0\n",
      "  (1902, 1887)\t1.0\n",
      "  (837, 1686)\t1.0\n"
     ]
    }
   ],
   "source": [
    "# 理解np.genfromtxt和sp.csr_matrix的处理：\n",
    "labels = encode_onehot(idx_features_labels[:, -1])\n",
    "idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "idx_map = {j: i for i, j in enumerate(idx)}\n",
    "edges_unordered = np.genfromtxt(\"{}{}.cites\".format(\"./cora/\", \"cora\"),dtype=np.int32)\n",
    "# print(edges_unordered.shape)\n",
    "# print(edges_unordered.flatten().shape)\n",
    "edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "# 为了格式能成功转换,此处需要调用.flatten()改变shape,最后再转回来\n",
    "print(edges_unordered)\n",
    "print(\"-\"*50)\n",
    "print(edges)\n",
    "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
    "print(adj)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(no_cuda=False, fastmode=True, seed=42, epochs=200, lr=0.01, weight_decay=0.0005, hidden=16, dropout=0.5)\n",
      "Loading cora dataset...\n",
      "Epoch:0001 loss_train:1.9151 acc_train:0.2571 loss_val:1.9009 acc_val:0.3167 time:0.0313\n",
      "Epoch:0002 loss_train:1.8888 acc_train:0.2929 loss_val:1.8776 acc_val:0.3500 time:0.0469\n",
      "Epoch:0003 loss_train:1.8784 acc_train:0.2929 loss_val:1.8618 acc_val:0.3500 time:0.0370\n",
      "Epoch:0004 loss_train:1.8702 acc_train:0.2929 loss_val:1.8597 acc_val:0.3500 time:0.0208\n",
      "Epoch:0005 loss_train:1.8407 acc_train:0.3071 loss_val:1.8464 acc_val:0.3500 time:0.0312\n",
      "Epoch:0006 loss_train:1.8350 acc_train:0.2929 loss_val:1.8418 acc_val:0.3500 time:0.0469\n",
      "Epoch:0007 loss_train:1.8315 acc_train:0.2857 loss_val:1.8287 acc_val:0.3500 time:0.0156\n",
      "Epoch:0008 loss_train:1.8077 acc_train:0.2929 loss_val:1.8176 acc_val:0.3433 time:0.0313\n",
      "Epoch:0009 loss_train:1.8076 acc_train:0.2857 loss_val:1.8048 acc_val:0.3500 time:0.0469\n",
      "Epoch:0010 loss_train:1.8045 acc_train:0.2929 loss_val:1.7954 acc_val:0.3500 time:0.0321\n",
      "Epoch:0011 loss_train:1.7922 acc_train:0.2929 loss_val:1.7979 acc_val:0.3500 time:0.0313\n",
      "Epoch:0012 loss_train:1.7903 acc_train:0.2929 loss_val:1.7997 acc_val:0.3500 time:0.0313\n",
      "Epoch:0013 loss_train:1.7908 acc_train:0.2929 loss_val:1.7900 acc_val:0.3467 time:0.0312\n",
      "Epoch:0014 loss_train:1.7619 acc_train:0.3000 loss_val:1.7897 acc_val:0.3567 time:0.0312\n",
      "Epoch:0015 loss_train:1.7557 acc_train:0.3000 loss_val:1.7810 acc_val:0.3533 time:0.0313\n",
      "Epoch:0016 loss_train:1.7612 acc_train:0.2929 loss_val:1.7787 acc_val:0.3500 time:0.0469\n",
      "Epoch:0017 loss_train:1.7370 acc_train:0.3357 loss_val:1.7680 acc_val:0.3467 time:0.0313\n",
      "Epoch:0018 loss_train:1.7369 acc_train:0.3143 loss_val:1.7628 acc_val:0.3600 time:0.0313\n",
      "Epoch:0019 loss_train:1.7257 acc_train:0.2857 loss_val:1.7707 acc_val:0.3433 time:0.0313\n",
      "Epoch:0020 loss_train:1.7287 acc_train:0.3429 loss_val:1.7566 acc_val:0.3600 time:0.0318\n",
      "Epoch:0021 loss_train:1.7195 acc_train:0.3643 loss_val:1.7559 acc_val:0.3500 time:0.0313\n",
      "Epoch:0022 loss_train:1.7068 acc_train:0.3429 loss_val:1.7442 acc_val:0.3500 time:0.0312\n",
      "Epoch:0023 loss_train:1.6980 acc_train:0.3286 loss_val:1.7376 acc_val:0.3567 time:0.0313\n",
      "Epoch:0024 loss_train:1.7099 acc_train:0.3643 loss_val:1.7431 acc_val:0.3467 time:0.0313\n",
      "Epoch:0025 loss_train:1.6924 acc_train:0.3357 loss_val:1.7350 acc_val:0.3833 time:0.0313\n",
      "Epoch:0026 loss_train:1.6926 acc_train:0.3714 loss_val:1.7357 acc_val:0.3800 time:0.0313\n",
      "Epoch:0027 loss_train:1.6672 acc_train:0.3714 loss_val:1.7387 acc_val:0.3533 time:0.0469\n",
      "Epoch:0028 loss_train:1.6450 acc_train:0.4214 loss_val:1.7185 acc_val:0.3733 time:0.0312\n",
      "Epoch:0029 loss_train:1.6439 acc_train:0.3643 loss_val:1.7084 acc_val:0.3733 time:0.0313\n",
      "Epoch:0030 loss_train:1.6494 acc_train:0.3929 loss_val:1.7150 acc_val:0.3600 time:0.0313\n",
      "Epoch:0031 loss_train:1.6172 acc_train:0.4786 loss_val:1.6826 acc_val:0.3800 time:0.0313\n",
      "Epoch:0032 loss_train:1.6140 acc_train:0.4286 loss_val:1.6840 acc_val:0.3900 time:0.0312\n",
      "Epoch:0033 loss_train:1.6076 acc_train:0.4143 loss_val:1.6888 acc_val:0.3800 time:0.0313\n",
      "Epoch:0034 loss_train:1.6061 acc_train:0.4286 loss_val:1.6805 acc_val:0.3733 time:0.0312\n",
      "Epoch:0035 loss_train:1.5770 acc_train:0.4429 loss_val:1.6649 acc_val:0.4033 time:0.0313\n",
      "Epoch:0036 loss_train:1.5706 acc_train:0.4643 loss_val:1.6650 acc_val:0.4100 time:0.0313\n",
      "Epoch:0037 loss_train:1.5492 acc_train:0.5000 loss_val:1.6514 acc_val:0.4300 time:0.0313\n",
      "Epoch:0038 loss_train:1.5297 acc_train:0.4643 loss_val:1.6420 acc_val:0.4167 time:0.0313\n",
      "Epoch:0039 loss_train:1.5250 acc_train:0.4857 loss_val:1.6365 acc_val:0.4367 time:0.0469\n",
      "Epoch:0040 loss_train:1.5071 acc_train:0.5071 loss_val:1.6344 acc_val:0.4200 time:0.0313\n",
      "Epoch:0041 loss_train:1.5124 acc_train:0.5071 loss_val:1.6362 acc_val:0.4400 time:0.0321\n",
      "Epoch:0042 loss_train:1.4945 acc_train:0.5214 loss_val:1.6169 acc_val:0.4333 time:0.0313\n",
      "Epoch:0043 loss_train:1.4768 acc_train:0.5643 loss_val:1.6057 acc_val:0.4867 time:0.0313\n",
      "Epoch:0044 loss_train:1.4542 acc_train:0.5429 loss_val:1.5699 acc_val:0.5100 time:0.0469\n",
      "Epoch:0045 loss_train:1.4587 acc_train:0.5429 loss_val:1.5904 acc_val:0.4500 time:0.0469\n",
      "Epoch:0046 loss_train:1.4375 acc_train:0.5643 loss_val:1.5710 acc_val:0.4967 time:0.0469\n",
      "Epoch:0047 loss_train:1.4023 acc_train:0.5714 loss_val:1.5636 acc_val:0.4700 time:0.0469\n",
      "Epoch:0048 loss_train:1.4097 acc_train:0.5929 loss_val:1.5481 acc_val:0.5133 time:0.0312\n",
      "Epoch:0049 loss_train:1.4004 acc_train:0.5643 loss_val:1.5359 acc_val:0.5100 time:0.0319\n",
      "Epoch:0050 loss_train:1.3781 acc_train:0.6286 loss_val:1.5201 acc_val:0.5367 time:0.0469\n",
      "Epoch:0051 loss_train:1.3844 acc_train:0.5857 loss_val:1.5215 acc_val:0.5333 time:0.0313\n",
      "Epoch:0052 loss_train:1.3567 acc_train:0.6143 loss_val:1.5217 acc_val:0.5267 time:0.0441\n",
      "Epoch:0053 loss_train:1.3385 acc_train:0.6071 loss_val:1.5008 acc_val:0.5333 time:0.0312\n",
      "Epoch:0054 loss_train:1.3148 acc_train:0.6357 loss_val:1.4770 acc_val:0.5333 time:0.0469\n",
      "Epoch:0055 loss_train:1.3194 acc_train:0.6857 loss_val:1.4797 acc_val:0.5400 time:0.0312\n",
      "Epoch:0056 loss_train:1.2566 acc_train:0.6857 loss_val:1.4714 acc_val:0.5500 time:0.0312\n",
      "Epoch:0057 loss_train:1.2903 acc_train:0.6643 loss_val:1.4692 acc_val:0.5500 time:0.0313\n",
      "Epoch:0058 loss_train:1.2875 acc_train:0.6214 loss_val:1.4607 acc_val:0.5867 time:0.0469\n",
      "Epoch:0059 loss_train:1.2606 acc_train:0.6571 loss_val:1.4504 acc_val:0.5733 time:0.0312\n",
      "Epoch:0060 loss_train:1.2445 acc_train:0.6571 loss_val:1.4514 acc_val:0.5767 time:0.0312\n",
      "Epoch:0061 loss_train:1.2122 acc_train:0.7071 loss_val:1.4391 acc_val:0.5567 time:0.0313\n",
      "Epoch:0062 loss_train:1.2065 acc_train:0.7000 loss_val:1.3832 acc_val:0.5833 time:0.0469\n",
      "Epoch:0063 loss_train:1.2080 acc_train:0.6857 loss_val:1.3673 acc_val:0.6233 time:0.0312\n",
      "Epoch:0064 loss_train:1.1938 acc_train:0.6786 loss_val:1.4069 acc_val:0.5833 time:0.0313\n",
      "Epoch:0065 loss_train:1.1225 acc_train:0.7429 loss_val:1.3525 acc_val:0.6133 time:0.0312\n",
      "Epoch:0066 loss_train:1.1552 acc_train:0.7071 loss_val:1.3632 acc_val:0.6033 time:0.0469\n",
      "Epoch:0067 loss_train:1.1612 acc_train:0.7357 loss_val:1.3712 acc_val:0.6000 time:0.0313\n",
      "Epoch:0068 loss_train:1.1192 acc_train:0.7286 loss_val:1.3427 acc_val:0.6500 time:0.0318\n",
      "Epoch:0069 loss_train:1.1048 acc_train:0.7571 loss_val:1.3263 acc_val:0.6500 time:0.0313\n",
      "Epoch:0070 loss_train:1.0835 acc_train:0.7143 loss_val:1.3015 acc_val:0.6500 time:0.0313\n",
      "Epoch:0071 loss_train:1.0507 acc_train:0.7643 loss_val:1.3115 acc_val:0.6533 time:0.0469\n",
      "Epoch:0072 loss_train:1.0887 acc_train:0.7571 loss_val:1.3005 acc_val:0.6300 time:0.0313\n",
      "Epoch:0073 loss_train:1.0607 acc_train:0.7571 loss_val:1.2872 acc_val:0.6633 time:0.0313\n",
      "Epoch:0074 loss_train:1.0471 acc_train:0.7500 loss_val:1.2692 acc_val:0.6600 time:0.0469\n",
      "Epoch:0075 loss_train:1.0334 acc_train:0.7429 loss_val:1.2571 acc_val:0.6300 time:0.0312\n",
      "Epoch:0076 loss_train:1.0249 acc_train:0.7929 loss_val:1.2679 acc_val:0.6433 time:0.0312\n",
      "Epoch:0077 loss_train:1.0116 acc_train:0.7286 loss_val:1.2288 acc_val:0.6633 time:0.0323\n",
      "Epoch:0078 loss_train:1.0204 acc_train:0.7571 loss_val:1.2626 acc_val:0.6533 time:0.0313\n",
      "Epoch:0079 loss_train:0.9380 acc_train:0.8286 loss_val:1.2563 acc_val:0.6767 time:0.0469\n",
      "Epoch:0080 loss_train:0.9614 acc_train:0.8000 loss_val:1.2066 acc_val:0.6933 time:0.0313\n",
      "Epoch:0081 loss_train:0.9261 acc_train:0.8357 loss_val:1.1958 acc_val:0.6967 time:0.0313\n",
      "Epoch:0082 loss_train:0.9536 acc_train:0.8000 loss_val:1.2182 acc_val:0.6633 time:0.0431\n",
      "Epoch:0083 loss_train:0.9354 acc_train:0.8000 loss_val:1.2082 acc_val:0.6967 time:0.0313\n",
      "Epoch:0084 loss_train:0.9399 acc_train:0.8071 loss_val:1.2036 acc_val:0.7133 time:0.0312\n",
      "Epoch:0085 loss_train:0.9392 acc_train:0.8286 loss_val:1.1859 acc_val:0.7300 time:0.0469\n",
      "Epoch:0086 loss_train:0.9174 acc_train:0.8000 loss_val:1.2055 acc_val:0.6667 time:0.0312\n",
      "Epoch:0087 loss_train:0.9222 acc_train:0.7929 loss_val:1.1843 acc_val:0.6733 time:0.0469\n",
      "Epoch:0088 loss_train:0.8776 acc_train:0.8071 loss_val:1.1709 acc_val:0.6867 time:0.0313\n",
      "Epoch:0089 loss_train:0.8890 acc_train:0.8071 loss_val:1.1471 acc_val:0.7200 time:0.0313\n",
      "Epoch:0090 loss_train:0.8492 acc_train:0.8357 loss_val:1.1393 acc_val:0.7133 time:0.0312\n",
      "Epoch:0091 loss_train:0.8750 acc_train:0.8214 loss_val:1.1840 acc_val:0.6800 time:0.0313\n",
      "Epoch:0092 loss_train:0.8574 acc_train:0.8143 loss_val:1.1273 acc_val:0.6933 time:0.0469\n",
      "Epoch:0093 loss_train:0.7925 acc_train:0.8286 loss_val:1.0978 acc_val:0.7233 time:0.0312\n",
      "Epoch:0094 loss_train:0.8298 acc_train:0.8214 loss_val:1.1403 acc_val:0.7033 time:0.0312\n",
      "Epoch:0095 loss_train:0.7944 acc_train:0.8571 loss_val:1.1003 acc_val:0.7167 time:0.0313\n",
      "Epoch:0096 loss_train:0.7990 acc_train:0.8286 loss_val:1.1176 acc_val:0.6867 time:0.0475\n",
      "Epoch:0097 loss_train:0.8207 acc_train:0.8143 loss_val:1.0971 acc_val:0.7233 time:0.0313\n",
      "Epoch:0098 loss_train:0.7919 acc_train:0.8357 loss_val:1.0926 acc_val:0.7200 time:0.0312\n",
      "Epoch:0099 loss_train:0.8104 acc_train:0.8571 loss_val:1.0738 acc_val:0.7100 time:0.0313\n",
      "Epoch:0100 loss_train:0.7658 acc_train:0.8500 loss_val:1.0599 acc_val:0.7100 time:0.0469\n",
      "Epoch:0101 loss_train:0.7262 acc_train:0.8857 loss_val:1.0612 acc_val:0.7233 time:0.0312\n",
      "Epoch:0102 loss_train:0.7977 acc_train:0.8286 loss_val:1.0189 acc_val:0.7400 time:0.0313\n",
      "Epoch:0103 loss_train:0.7308 acc_train:0.8643 loss_val:1.0433 acc_val:0.7167 time:0.0305\n",
      "Epoch:0104 loss_train:0.7131 acc_train:0.8571 loss_val:1.0404 acc_val:0.7167 time:0.0469\n",
      "Epoch:0105 loss_train:0.7014 acc_train:0.8643 loss_val:1.0108 acc_val:0.7400 time:0.0313\n",
      "Epoch:0106 loss_train:0.7361 acc_train:0.8786 loss_val:1.0295 acc_val:0.7467 time:0.0389\n",
      "Epoch:0107 loss_train:0.6880 acc_train:0.8857 loss_val:1.0297 acc_val:0.7400 time:0.0312\n",
      "Epoch:0108 loss_train:0.7078 acc_train:0.8429 loss_val:0.9842 acc_val:0.7667 time:0.0313\n",
      "Epoch:0109 loss_train:0.7188 acc_train:0.8500 loss_val:1.0539 acc_val:0.7367 time:0.0469\n",
      "Epoch:0110 loss_train:0.6943 acc_train:0.8357 loss_val:1.0798 acc_val:0.6933 time:0.0313\n",
      "Epoch:0111 loss_train:0.7304 acc_train:0.8714 loss_val:1.0608 acc_val:0.7167 time:0.0313\n",
      "Epoch:0112 loss_train:0.6592 acc_train:0.8929 loss_val:1.0251 acc_val:0.7233 time:0.0313\n",
      "Epoch:0113 loss_train:0.6827 acc_train:0.8714 loss_val:1.0330 acc_val:0.7233 time:0.0312\n",
      "Epoch:0114 loss_train:0.7241 acc_train:0.8786 loss_val:1.0127 acc_val:0.7367 time:0.0313\n",
      "Epoch:0115 loss_train:0.6782 acc_train:0.9000 loss_val:1.0223 acc_val:0.7367 time:0.0469\n",
      "Epoch:0116 loss_train:0.6123 acc_train:0.9000 loss_val:0.9387 acc_val:0.7667 time:0.0313\n",
      "Epoch:0117 loss_train:0.6478 acc_train:0.8857 loss_val:0.9904 acc_val:0.7367 time:0.0313\n",
      "Epoch:0118 loss_train:0.6681 acc_train:0.8643 loss_val:0.9713 acc_val:0.7433 time:0.0313\n",
      "Epoch:0119 loss_train:0.6480 acc_train:0.8857 loss_val:0.9614 acc_val:0.7767 time:0.0469\n",
      "Epoch:0120 loss_train:0.6631 acc_train:0.8786 loss_val:0.9694 acc_val:0.7567 time:0.0312\n",
      "Epoch:0121 loss_train:0.6493 acc_train:0.8929 loss_val:0.9912 acc_val:0.7533 time:0.0436\n",
      "Epoch:0122 loss_train:0.6389 acc_train:0.8714 loss_val:0.9527 acc_val:0.7567 time:0.0312\n",
      "Epoch:0123 loss_train:0.6177 acc_train:0.8786 loss_val:0.9690 acc_val:0.7300 time:0.0469\n",
      "Epoch:0124 loss_train:0.6339 acc_train:0.8643 loss_val:0.9758 acc_val:0.7367 time:0.0319\n",
      "Epoch:0125 loss_train:0.6768 acc_train:0.8357 loss_val:0.9589 acc_val:0.7267 time:0.0469\n",
      "Epoch:0126 loss_train:0.6352 acc_train:0.8643 loss_val:0.9465 acc_val:0.7167 time:0.0313\n",
      "Epoch:0127 loss_train:0.6409 acc_train:0.8786 loss_val:0.9539 acc_val:0.7467 time:0.0312\n",
      "Epoch:0128 loss_train:0.6130 acc_train:0.9000 loss_val:0.9536 acc_val:0.7533 time:0.0469\n",
      "Epoch:0129 loss_train:0.6495 acc_train:0.8571 loss_val:1.0008 acc_val:0.7400 time:0.0313\n",
      "Epoch:0130 loss_train:0.6454 acc_train:0.8786 loss_val:0.9891 acc_val:0.7500 time:0.0469\n",
      "Epoch:0131 loss_train:0.6417 acc_train:0.8357 loss_val:0.9423 acc_val:0.7333 time:0.0313\n",
      "Epoch:0132 loss_train:0.5806 acc_train:0.9286 loss_val:0.9596 acc_val:0.7433 time:0.0312\n",
      "Epoch:0133 loss_train:0.5849 acc_train:0.8714 loss_val:0.9382 acc_val:0.7367 time:0.0324\n",
      "Epoch:0134 loss_train:0.5901 acc_train:0.8929 loss_val:0.9306 acc_val:0.7467 time:0.0313\n",
      "Epoch:0135 loss_train:0.6261 acc_train:0.8786 loss_val:0.9268 acc_val:0.7567 time:0.0312\n",
      "Epoch:0136 loss_train:0.5978 acc_train:0.8857 loss_val:0.9186 acc_val:0.7533 time:0.0274\n",
      "Epoch:0137 loss_train:0.5782 acc_train:0.8857 loss_val:0.9590 acc_val:0.7133 time:0.0469\n",
      "Epoch:0138 loss_train:0.5600 acc_train:0.8929 loss_val:0.8999 acc_val:0.7567 time:0.0313\n",
      "Epoch:0139 loss_train:0.5709 acc_train:0.9000 loss_val:0.9254 acc_val:0.7433 time:0.0313\n",
      "Epoch:0140 loss_train:0.6299 acc_train:0.8857 loss_val:0.9643 acc_val:0.7300 time:0.0312\n",
      "Epoch:0141 loss_train:0.5881 acc_train:0.8929 loss_val:0.9380 acc_val:0.7667 time:0.0313\n",
      "Epoch:0142 loss_train:0.5592 acc_train:0.8857 loss_val:0.9674 acc_val:0.7133 time:0.0469\n",
      "Epoch:0143 loss_train:0.5463 acc_train:0.9000 loss_val:0.8940 acc_val:0.7500 time:0.0313\n",
      "Epoch:0144 loss_train:0.5322 acc_train:0.9214 loss_val:0.8927 acc_val:0.7367 time:0.0312\n",
      "Epoch:0145 loss_train:0.5234 acc_train:0.9071 loss_val:0.8792 acc_val:0.7467 time:0.0469\n",
      "Epoch:0146 loss_train:0.5682 acc_train:0.9143 loss_val:0.9088 acc_val:0.7600 time:0.0313\n",
      "Epoch:0147 loss_train:0.5668 acc_train:0.9071 loss_val:0.9586 acc_val:0.7033 time:0.0313\n",
      "Epoch:0148 loss_train:0.5764 acc_train:0.9000 loss_val:0.9358 acc_val:0.7367 time:0.0312\n",
      "Epoch:0149 loss_train:0.5632 acc_train:0.9143 loss_val:0.8734 acc_val:0.7533 time:0.0313\n",
      "Epoch:0150 loss_train:0.5654 acc_train:0.8929 loss_val:0.9321 acc_val:0.7367 time:0.0469\n",
      "Epoch:0151 loss_train:0.5454 acc_train:0.8714 loss_val:0.8715 acc_val:0.7733 time:0.0312\n",
      "Epoch:0152 loss_train:0.5352 acc_train:0.9143 loss_val:0.8956 acc_val:0.7433 time:0.0313\n",
      "Epoch:0153 loss_train:0.5279 acc_train:0.9286 loss_val:0.9550 acc_val:0.7300 time:0.0324\n",
      "Epoch:0154 loss_train:0.5445 acc_train:0.9143 loss_val:0.8670 acc_val:0.7833 time:0.0313\n",
      "Epoch:0155 loss_train:0.5363 acc_train:0.9143 loss_val:0.8966 acc_val:0.7567 time:0.0469\n",
      "Epoch:0156 loss_train:0.5193 acc_train:0.9000 loss_val:0.8258 acc_val:0.7800 time:0.0313\n",
      "Epoch:0157 loss_train:0.4759 acc_train:0.9286 loss_val:0.8784 acc_val:0.7533 time:0.0312\n",
      "Epoch:0158 loss_train:0.5252 acc_train:0.9357 loss_val:0.8737 acc_val:0.7467 time:0.0428\n",
      "Epoch:0159 loss_train:0.4939 acc_train:0.9500 loss_val:0.9060 acc_val:0.7500 time:0.0313\n",
      "Epoch:0160 loss_train:0.5106 acc_train:0.9500 loss_val:0.8634 acc_val:0.7633 time:0.0313\n",
      "Epoch:0161 loss_train:0.5309 acc_train:0.8857 loss_val:0.8882 acc_val:0.7200 time:0.0312\n",
      "Epoch:0162 loss_train:0.5029 acc_train:0.9000 loss_val:0.9104 acc_val:0.7533 time:0.0475\n",
      "Epoch:0163 loss_train:0.5082 acc_train:0.9143 loss_val:0.8398 acc_val:0.7933 time:0.0313\n",
      "Epoch:0164 loss_train:0.4822 acc_train:0.9000 loss_val:0.8881 acc_val:0.7300 time:0.0312\n",
      "Epoch:0165 loss_train:0.5172 acc_train:0.9143 loss_val:0.9177 acc_val:0.7400 time:0.0313\n",
      "Epoch:0166 loss_train:0.5592 acc_train:0.9071 loss_val:0.8685 acc_val:0.7400 time:0.0469\n",
      "Epoch:0167 loss_train:0.4790 acc_train:0.8786 loss_val:0.8589 acc_val:0.7533 time:0.0313\n",
      "Epoch:0168 loss_train:0.4896 acc_train:0.9143 loss_val:0.8453 acc_val:0.7733 time:0.0469\n",
      "Epoch:0169 loss_train:0.4746 acc_train:0.9357 loss_val:0.8390 acc_val:0.7933 time:0.0312\n",
      "Epoch:0170 loss_train:0.5258 acc_train:0.8929 loss_val:0.8662 acc_val:0.7600 time:0.0313\n",
      "Epoch:0171 loss_train:0.4801 acc_train:0.9214 loss_val:0.8759 acc_val:0.7733 time:0.0313\n",
      "Epoch:0172 loss_train:0.4755 acc_train:0.9500 loss_val:0.8843 acc_val:0.7333 time:0.0313\n",
      "Epoch:0173 loss_train:0.5116 acc_train:0.8857 loss_val:0.8883 acc_val:0.7300 time:0.0313\n",
      "Epoch:0174 loss_train:0.5158 acc_train:0.9143 loss_val:0.8300 acc_val:0.7600 time:0.0469\n",
      "Epoch:0175 loss_train:0.4889 acc_train:0.9214 loss_val:0.8074 acc_val:0.7667 time:0.0312\n",
      "Epoch:0176 loss_train:0.4927 acc_train:0.9286 loss_val:0.8677 acc_val:0.7567 time:0.0313\n",
      "Epoch:0177 loss_train:0.4636 acc_train:0.9143 loss_val:0.8397 acc_val:0.7233 time:0.0312\n",
      "Epoch:0178 loss_train:0.5116 acc_train:0.9071 loss_val:0.9094 acc_val:0.7267 time:0.0313\n",
      "Epoch:0179 loss_train:0.4644 acc_train:0.9500 loss_val:0.8931 acc_val:0.7400 time:0.0313\n",
      "Epoch:0180 loss_train:0.4531 acc_train:0.9286 loss_val:0.8466 acc_val:0.7633 time:0.0313\n",
      "Epoch:0181 loss_train:0.4409 acc_train:0.9500 loss_val:0.8701 acc_val:0.7267 time:0.0313\n",
      "Epoch:0182 loss_train:0.5056 acc_train:0.9214 loss_val:0.8733 acc_val:0.7367 time:0.0475\n",
      "Epoch:0183 loss_train:0.4849 acc_train:0.9071 loss_val:0.8853 acc_val:0.7500 time:0.0312\n",
      "Epoch:0184 loss_train:0.4647 acc_train:0.9214 loss_val:0.8537 acc_val:0.7567 time:0.0469\n",
      "Epoch:0185 loss_train:0.4593 acc_train:0.9143 loss_val:0.8302 acc_val:0.7667 time:0.0313\n",
      "Epoch:0186 loss_train:0.4433 acc_train:0.9286 loss_val:0.8159 acc_val:0.7733 time:0.0313\n",
      "Epoch:0187 loss_train:0.4895 acc_train:0.9000 loss_val:0.8382 acc_val:0.7633 time:0.0313\n",
      "Epoch:0188 loss_train:0.4400 acc_train:0.9286 loss_val:0.8407 acc_val:0.7567 time:0.0312\n",
      "Epoch:0189 loss_train:0.4608 acc_train:0.9000 loss_val:0.8581 acc_val:0.7600 time:0.0313\n",
      "Epoch:0190 loss_train:0.4409 acc_train:0.9357 loss_val:0.8502 acc_val:0.7700 time:0.0469\n",
      "Epoch:0191 loss_train:0.4494 acc_train:0.9357 loss_val:0.8339 acc_val:0.7700 time:0.0335\n",
      "Epoch:0192 loss_train:0.4498 acc_train:0.9214 loss_val:0.8201 acc_val:0.7600 time:0.0313\n",
      "Epoch:0193 loss_train:0.4238 acc_train:0.9357 loss_val:0.8159 acc_val:0.7900 time:0.0312\n",
      "Epoch:0194 loss_train:0.4773 acc_train:0.9000 loss_val:0.8336 acc_val:0.7767 time:0.0433\n",
      "Epoch:0195 loss_train:0.4611 acc_train:0.9357 loss_val:0.8486 acc_val:0.7733 time:0.0313\n",
      "Epoch:0196 loss_train:0.4533 acc_train:0.9214 loss_val:0.8218 acc_val:0.7567 time:0.0313\n",
      "Epoch:0197 loss_train:0.4249 acc_train:0.9571 loss_val:0.8335 acc_val:0.7800 time:0.0313\n",
      "Epoch:0198 loss_train:0.4097 acc_train:0.9429 loss_val:0.8517 acc_val:0.7533 time:0.0469\n",
      "Epoch:0199 loss_train:0.4418 acc_train:0.9071 loss_val:0.8487 acc_val:0.7433 time:0.0312\n",
      "Epoch:0200 loss_train:0.4352 acc_train:0.9571 loss_val:0.7966 acc_val:0.7933 time:0.0313\n",
      "Total time:6.9929s\n",
      "Test set results: loss=0.7787 accuracy=0.8110\n"
     ]
    }
   ],
   "source": [
    "# from __future__ import division\n",
    "# from __future__ import print_function\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--no-cuda\", action=\"store_true\", default=False, help=\"Disables CUDA training.\")\n",
    "parser.add_argument(\"--fastmode\", action=\"store_true\", default=True, help=\"Validate during training pass.\")\n",
    "parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed.\")\n",
    "parser.add_argument(\"--epochs\", type=int, default=200, help=\"Number of epochs to train.\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.01, help=\"Initial learning rate.\")\n",
    "parser.add_argument(\"--weight_decay\", type=float, default=5e-4, help=\"Weight decay(L2 loss on parameters).\")\n",
    "parser.add_argument(\"--hidden\", type=int, default=16, help=\"Number of hidden units\")\n",
    "parser.add_argument(\"--dropout\", type=float, default=0.5, help=\"Dropout rate (1 - keep probability).\")\n",
    "# args = parser.parse_args()\n",
    "args =parser.parse_known_args()[0]\n",
    "# print(parser)\n",
    "print(args)\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "np.random.seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
    "model = GCN(nfeat=features.shape[1], nhid=args.hidden, nclass=labels.max().item()+1, dropout=args.dropout)\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not args.fastmode:\n",
    "        model.eval() # 注意：dropout会影响前向传播,从而影响预测结果\n",
    "        output = model(features, adj)\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print(\"Epoch:{:04d}\".format(epoch+1), \"loss_train:{:.4f}\".format(loss_train.item()), \"acc_train:{:.4f}\".format(acc_train.item()), \"loss_val:{:.4f}\".format(loss_val.item()), \"acc_val:{:.4f}\".format(acc_val.item()), \"time:{:.4f}\".format(time.time()-t))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\", \"loss={:.4f}\".format(loss_test.item()), \"accuracy={:.4f}\".format(acc_test.item()))\n",
    "\n",
    "t_total = time.time()\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch)\n",
    "print(\"Total time:{:.4f}s\".format(time.time()-t_total))\n",
    "\n",
    "test()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}