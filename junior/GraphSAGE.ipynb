{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "\n",
    "class MeanAggregator(nn.Module):\n",
    "    def __init__(self, features, cuda=False, gcn=False):\n",
    "        super(MeanAggregator, self).__init__()\n",
    "        self.features = features\n",
    "        self.cuda = cuda\n",
    "        self.gcn = gcn\n",
    "\n",
    "    def forward(self, nodes, to_neighs, num_sample=10):\n",
    "        _set = set\n",
    "        if not num_sample is None:\n",
    "            _sample = random.sample\n",
    "            samp_neighs = [_set(_sample(to_neigh, num_sample)) if len(to_neigh) >= num_sample else to_neigh for to_neigh in to_neighs]\n",
    "            # 注意：此处先执行前面的判断语句，后执行for to_neigh in to_neighs形成list\n",
    "        else:\n",
    "            samp_neighs = to_neighs\n",
    "        if self.gcn:\n",
    "            samp_neighs = [set.union(samp_neigh, _set([nodes[i]])) for i, samp_neigh in enumerate(samp_neighs)]\n",
    "        unique_nodes_list = list(set.union(*samp_neighs))\n",
    "        unique_nodes = {n:i for i, n in enumerate(unique_nodes_list)}\n",
    "        mask = Variable(torch.zeros(len(samp_neighs), len(unique_nodes)))\n",
    "        column_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh]\n",
    "        row_indices = [i for i in range(len(samp_neighs)) for j in range(len(samp_neighs[i]))]\n",
    "        mask[row_indices, column_indices] = 1\n",
    "        if self.cuda:\n",
    "            mask = mask.cuda()\n",
    "        num_neigh = mask.sum(1, keepdims=True)\n",
    "        mask = mask.div(num_neigh)\n",
    "        if self.cuda:\n",
    "            embed_matrix = self.features(torch.LongTensor(unique_nodes_list).cuda())\n",
    "        else:\n",
    "            embed_matrix = self.features(torch.LongTensor(unique_nodes_list))\n",
    "        to_feats = mask.mm(embed_matrix)\n",
    "        return to_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{1, 2, 5}]\n"
     ]
    }
   ],
   "source": [
    "# 理解关于set的随机采样\n",
    "a = set\n",
    "b = random.sample\n",
    "c = [1,2,3,4,5]\n",
    "d = [a(b(c,3))]\n",
    "print(d)\n",
    "# print([a(c)+a(c)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{1, 2, 3, 4, 5}, {2, 3, 4, 5, 6}]\n",
      "[1, 2, 3, 4, 5, 6]\n",
      "{1, 2, 3, 4, 5, 7}\n"
     ]
    }
   ],
   "source": [
    "# 理解set的并集操作\n",
    "e = [2,3,4,5,6]\n",
    "f = [a(c), a(e)]\n",
    "print(f)\n",
    "g = list(set.union(*f))\n",
    "print(g)\n",
    "h = set.union(a(c), a([7]))\n",
    "print(h)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, features, feature_dim, embed_dim, adj_lists, aggregator, num_sample=10, base_model=None, gcn=False, cuda=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.features = features\n",
    "        self.feat_dim = feature_dim\n",
    "        self.adj_lists = adj_lists\n",
    "        self.aggregator = aggregator\n",
    "        self.num_sample = num_sample\n",
    "        if base_model != None:\n",
    "            self.base_model = base_model\n",
    "        self.gcn = gcn\n",
    "        self.embed_dim = embed_dim\n",
    "        self.cuda = cuda\n",
    "        self.aggregator.cuda = cuda\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(embed_dim, self.feat_dim if self.gcn else 2*self.feat_dim))\n",
    "        # 注意：此处将\",\"后面的语句作为整体执行\n",
    "        init.xavier_uniform_(self.weight) # 初始化操作\n",
    "\n",
    "    def forward(self, nodes):\n",
    "        # neigh_feats = self.aggregator.forward(nodes, self.adj_lists, self.num_sample)\n",
    "        neigh_feats = self.aggregator.forward(nodes, [self.adj_lists[int(node)] for node in nodes], self.num_sample)\n",
    "        if not self.gcn:\n",
    "            if self.cuda:\n",
    "                self_feats = self.features(torch.LongTensor(nodes).cuda())\n",
    "            else:\n",
    "                self_feats = self.features(torch.LongTensor(nodes))\n",
    "                # print(self_feats.shape) # (34, 16)\n",
    "            combined = torch.cat([self_feats, neigh_feats], dim=1)\n",
    "            # print(combined.shape) # (34, 32)\n",
    "        else:\n",
    "            combined = neigh_feats\n",
    "        # print(combined.shape) # (34, 32)\n",
    "        # print(self.weight.shape) # (16, 32)\n",
    "        combined = F.relu(self.weight.mm(combined.t()))\n",
    "        # print(combined.shape) # (16, 34)\n",
    "        return combined"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "from torch.nn import Linear\n",
    "\n",
    "class SupervisedGraphSage(nn.Module):\n",
    "    def __init__(self, num_classes, enc):\n",
    "        super(SupervisedGraphSage, self).__init__()\n",
    "        self.enc = enc\n",
    "        self.xent = nn.CrossEntropyLoss()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(num_classes, enc.embed_dim))\n",
    "        init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, nodes):\n",
    "        embeds = self.enc(nodes)\n",
    "        # print(embeds.shape) # (16, 34)\n",
    "        # print(self.weight.shape) # (4, 16)\n",
    "        scores = self.weight.mm(embeds)\n",
    "        # print(scores.shape) # (4, 34)\n",
    "        # return scores.t() # (34, 4)\n",
    "        scores_softmax = torch.exp(scores.t())/torch.sum(torch.exp(scores.t()), dim=1).reshape(-1, 1)\n",
    "        # print(scores_softmax.shape)\n",
    "        # print(scores_softmax.sum(dim=1))\n",
    "        return scores_softmax # (34, 4)\n",
    "\n",
    "    def loss(self, nodes, labels):\n",
    "        scores = self.forward(nodes)\n",
    "        # print(scores.sum(dim=1))\n",
    "        return self.xent(scores, labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def get_to_neighs(G):\n",
    "    num_nodes = len(G.nodes)\n",
    "    to_neighs = []\n",
    "    for i in range(num_nodes):\n",
    "        mid_set = set()\n",
    "        for j in G.neighbors(i):\n",
    "            mid_set.add(j)\n",
    "        to_neighs.append(mid_set)\n",
    "    return to_neighs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 3, 3, 3, 1, 0, 1, 3, 1, 1, 1, 0, 0, 3, 1, 0, 1, 0, 1, 0, 0,\n",
      "        2, 2, 0, 0, 2, 0, 0, 2, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# def get_batch_nodes(graph):\n",
    "#     batch_nodes = []\n",
    "#     num_nodes = len(graph.nodes)\n",
    "#     pos = nx.spring_layout(graph)\n",
    "#     for i in range(num_nodes):\n",
    "#         batch_nodes.append(list(pos[i]))\n",
    "#     return batch_nodes\n",
    "\n",
    "G = nx.karate_club_graph()\n",
    "# batch_nodes = get_batch_nodes(G)\n",
    "# print(batch_nodes)\n",
    "\n",
    "from torch_geometric.datasets import KarateClub\n",
    "dataset = KarateClub()\n",
    "data = dataset[0]\n",
    "labels = data.y\n",
    "print(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def create_elements(graph = nx.karate_club_graph(), num_embed = 34, feature_dim = 16, embed_dim = 16, num_classes = 4, nodes=None):\n",
    "    if nodes == None:\n",
    "        nodes = []\n",
    "        for i in range(len(graph.nodes)):\n",
    "            nodes.append(i)\n",
    "    split = int(len(nodes)/3*2)\n",
    "    features = nn.Embedding(num_embed, embed_dim)\n",
    "    to_neighs = get_to_neighs(graph)\n",
    "    aggregator = MeanAggregator(features, nodes[:split], to_neighs)\n",
    "    enc = Encoder(features=features, feature_dim=feature_dim, embed_dim=embed_dim, adj_lists=to_neighs, aggregator=aggregator)\n",
    "    model = SupervisedGraphSage(num_classes=num_classes, enc=enc)\n",
    "    # criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    return nodes, to_neighs, model, optimizer, features, split\n",
    "\n",
    "def train(nodes, labels, model, optimizer):\n",
    "    times = []\n",
    "    losses = []\n",
    "    for i in range(100):\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(nodes, labels)\n",
    "        losses.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        end_time = time.time()\n",
    "        times.append(end_time-start_time)\n",
    "    # print(times)\n",
    "    # print(losses)\n",
    "\n",
    "def predict(nodes, labels, model):\n",
    "    after_model = model(nodes)\n",
    "    predicted = torch.argmax(after_model, dim=1)\n",
    "    acc = (torch.sum(predicted == labels) / len(labels)).item()\n",
    "    print(acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "数据集：nx.karate_club_graph()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5833333134651184\n"
     ]
    }
   ],
   "source": [
    "nodes, to_neights, model, optimizer, features,split = create_elements(graph = nx.karate_club_graph(), num_embed = 34, feature_dim = 16, embed_dim = 16, num_classes = 4, nodes=None)\n",
    "train(nodes=nodes[:split], labels=labels[:split], model=model, optimizer=optimizer)\n",
    "predict(nodes=nodes[split:], labels=labels[split:], model=model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "数据集：nx.read_edgelist(\"./facebook/414.edges\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 1693\n",
      "0.9300000071525574\n"
     ]
    }
   ],
   "source": [
    "G_fb = nx.read_edgelist(\"./facebook/414.edges\")\n",
    "n = G_fb.number_of_nodes()\n",
    "m = G_fb.number_of_edges()\n",
    "print(n, m)\n",
    "mapping = dict(zip(G_fb.nodes(), range(n)))\n",
    "# print(mapping)\n",
    "nx.relabel_nodes(G_fb, mapping, copy=False)\n",
    "# print(G_fb.nodes)\n",
    "\n",
    "nodes_fb, to_neights_fb, model_fb, optimizer_fb, features_fb, split_fb = create_elements(graph = G_fb, num_embed = 150, feature_dim = 16, embed_dim = 16, num_classes = 11, nodes=list(G_fb.nodes))\n",
    "labels_fb = torch.tensor([random.randint(0, 10) for i in range(150)])\n",
    "train(nodes=nodes_fb[:split_fb], labels=labels_fb[:split_fb], model=model_fb, optimizer=optimizer_fb)\n",
    "predict(nodes=nodes_fb[:split_fb], labels=labels_fb[:split_fb], model=model_fb)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "数据集：cora"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([34, 128])\n",
      "0 1.9465709924697876\n",
      "1 1.9443682432174683\n",
      "2 1.9437133073806763\n",
      "3 1.9408988952636719\n",
      "4 1.9394015073776245\n",
      "5 1.9376933574676514\n",
      "6 1.934814691543579\n",
      "7 1.933018684387207\n",
      "8 1.9312365055084229\n",
      "9 1.9276008605957031\n",
      "10 1.9262861013412476\n",
      "11 1.923661470413208\n",
      "12 1.9221779108047485\n",
      "13 1.9135255813598633\n",
      "14 1.9109102487564087\n",
      "15 1.900695562362671\n",
      "16 1.8865468502044678\n",
      "17 1.8605780601501465\n",
      "18 1.8569773435592651\n",
      "19 1.8515504598617554\n",
      "20 1.8274928331375122\n",
      "21 1.8302457332611084\n",
      "22 1.8250945806503296\n",
      "23 1.8270838260650635\n",
      "24 1.8249050378799438\n",
      "25 1.7910780906677246\n",
      "26 1.7972108125686646\n",
      "27 1.7873475551605225\n",
      "28 1.7802425622940063\n",
      "29 1.7381370067596436\n",
      "30 1.7761728763580322\n",
      "31 1.8076591491699219\n",
      "32 1.7605476379394531\n",
      "33 1.7475898265838623\n",
      "34 1.7730965614318848\n",
      "35 1.7561051845550537\n",
      "36 1.7442009449005127\n",
      "37 1.7295632362365723\n",
      "38 1.7497880458831787\n",
      "39 1.7061796188354492\n",
      "40 1.7006193399429321\n",
      "41 1.6733025312423706\n",
      "42 1.6384819746017456\n",
      "43 1.6972404718399048\n",
      "44 1.6514227390289307\n",
      "45 1.6591951847076416\n",
      "46 1.6485956907272339\n",
      "47 1.6227772235870361\n",
      "48 1.6249513626098633\n",
      "49 1.62684965133667\n",
      "50 1.595320701599121\n",
      "51 1.6207953691482544\n",
      "52 1.6055904626846313\n",
      "53 1.5890631675720215\n",
      "54 1.5745936632156372\n",
      "55 1.595600962638855\n",
      "56 1.568199634552002\n",
      "57 1.5626816749572754\n",
      "58 1.5315916538238525\n",
      "59 1.525620937347412\n",
      "60 1.5541307926177979\n",
      "61 1.5654159784317017\n",
      "62 1.522416353225708\n",
      "63 1.4988539218902588\n",
      "64 1.5584158897399902\n",
      "65 1.5064209699630737\n",
      "66 1.5104308128356934\n",
      "67 1.475993037223816\n",
      "68 1.4671902656555176\n",
      "69 1.4888893365859985\n",
      "70 1.4656524658203125\n",
      "71 1.4603116512298584\n",
      "72 1.4484732151031494\n",
      "73 1.433800220489502\n",
      "74 1.45119047164917\n",
      "75 1.438954472541809\n",
      "76 1.4222519397735596\n",
      "77 1.3797948360443115\n",
      "78 1.42026948928833\n",
      "79 1.4310970306396484\n",
      "80 1.3816252946853638\n",
      "81 1.3993438482284546\n",
      "82 1.3738489151000977\n",
      "83 1.363796591758728\n",
      "84 1.3639380931854248\n",
      "85 1.3721610307693481\n",
      "86 1.3617557287216187\n",
      "87 1.3479937314987183\n",
      "88 1.3208757638931274\n",
      "89 1.3430325984954834\n",
      "90 1.3357446193695068\n",
      "91 1.346467137336731\n",
      "92 1.3666789531707764\n",
      "93 1.3282195329666138\n",
      "94 1.3388093709945679\n",
      "95 1.3546119928359985\n",
      "96 1.313551425933838\n",
      "97 1.3192875385284424\n",
      "98 1.3438713550567627\n",
      "99 1.3333895206451416\n",
      "Validation F1: 0.7940000000000002\n",
      "Average batch time: 0.08241229295730591\n"
     ]
    }
   ],
   "source": [
    "def load_cora():\n",
    "    num_nodes = 2708\n",
    "    num_feats = 1433\n",
    "    feat_data = np.zeros((num_nodes, num_feats)) # 此处不能用torch代替np\n",
    "    labels = np.empty((num_nodes, 1), dtype=np.int64) # 此处不能用torch代替np\n",
    "    node_map = {}\n",
    "    label_map = {}\n",
    "    with open(\"./cora/cora.content\") as fp:\n",
    "        for i, line in enumerate(fp):\n",
    "            info = line.strip().split()\n",
    "            info_label = info[-1]\n",
    "            info = info[:-1]\n",
    "            info = [int(x) for x in info]\n",
    "            feat_data[i, :] = info[1:]\n",
    "            # print(len(feat_data[i, :]))\n",
    "            node_map[info[0]] = i\n",
    "            if not info_label in label_map:\n",
    "                label_map[info_label] = len(label_map) # len({}) = 1\n",
    "            labels[i] = label_map[info_label]\n",
    "    adj_lists = defaultdict(set) # 注意：此处不能用普通的set\n",
    "    # print(adj_lists)\n",
    "    # print(node_map)\n",
    "    # print(label_map)\n",
    "    # print(labels.shape)\n",
    "    with open(\"./cora/cora.cites\") as fp:\n",
    "        for i, line in enumerate(fp):\n",
    "            info = line.strip().split()\n",
    "            info = [int(x) for x in info]\n",
    "            paper1 = node_map[info[0]]\n",
    "            paper2 = node_map[info[1]]\n",
    "            adj_lists[paper1].add(paper2)\n",
    "            adj_lists[paper2].add(paper1)\n",
    "    return feat_data, labels, adj_lists\n",
    "\n",
    "# def run_cora():\n",
    "np.random.seed(1) # 应对用np.random生成的随机数\n",
    "random.seed(1) # 应对用random生成的随机数\n",
    "num_nodes = 2708\n",
    "num_feats = 1433\n",
    "feat_data, labels, adj_lists = load_cora()\n",
    "features = nn.Embedding(num_nodes, num_feats) # torch.LongTensor等价于torch.tensor\n",
    "features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=False) # torch.FloatTensor等价于torch.Tensor\n",
    "# 上一行的作用：加快训练速度，提升训练效果(F1)\n",
    "# print(features)\n",
    "# print(features.weight.sum(dim=1))\n",
    "\n",
    "agg1 = MeanAggregator(features)\n",
    "enc1 = Encoder(features=features, feature_dim=1433, embed_dim=128, adj_lists=adj_lists, aggregator=agg1) # gcn=False\n",
    "print(enc1(nodes).t().shape)\n",
    "agg2 = MeanAggregator(features=lambda nodes : enc1(nodes).t())\n",
    "enc2 = Encoder(features=lambda nodes : enc1(nodes).t(), feature_dim=enc1.embed_dim, embed_dim=128, adj_lists=adj_lists, aggregator=agg2, base_model=enc1) # gcn=False\n",
    "\n",
    "graphsage = SupervisedGraphSage(7, enc2)\n",
    "rand_indices = np.random.permutation(num_nodes)\n",
    "# print(num_nodes)\n",
    "# print(rand_indices)\n",
    "test = rand_indices[:1000]\n",
    "val = rand_indices[1000:1500]\n",
    "train = list(rand_indices[1500:])\n",
    "optimizer = torch.optim.SGD(filter(lambda p : p.requires_grad, graphsage.parameters()), lr=0.7)\n",
    "# parameters = filter(lambda p : p.requires_grad, graphsage.parameters())\n",
    "# print(parameters)\n",
    "# print(graphsage.parameters())\n",
    "times = []\n",
    "for batch in range(100):\n",
    "    batch_nodes = train[:256]\n",
    "    random.shuffle(train)\n",
    "    start_time = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    loss = graphsage.loss(batch_nodes, torch.LongTensor(labels[np.array(batch_nodes)].reshape(1, -1)[0]))# Variable\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    end_time = time.time()\n",
    "    times.append(end_time-start_time)\n",
    "    print(batch, loss.item())\n",
    "\n",
    "# print(labels[val])\n",
    "# print(labels[train]) # 二者在格式上没有区别\n",
    "val_output = graphsage.forward(val)\n",
    "# print(val_output.data.numpy().argmax(axis=1))\n",
    "# print(\"-\"*50)\n",
    "# print(val_output.argmax(axis=1))\n",
    "print(\"Validation F1:\", f1_score(labels[val], val_output.argmax(axis=1), average=\"micro\"))\n",
    "print(\"Average batch time:\", np.mean(times))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "下面均为帮助理解整个网络架构的测试代码"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "to_neighs = get_to_neighs(G)\n",
    "samp_neighs = [set(random.sample(to_neigh, 3)) if len(to_neigh) >= 3 else to_neigh for to_neigh in to_neighs]\n",
    "# print(samp_neighs)\n",
    "samp_neighs = [set.union(samp_neigh, set([nodes[i]])) for i, samp_neigh in enumerate(samp_neighs)]\n",
    "# print(samp_neighs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "unique_nodes_list = list(set.union(*samp_neighs))\n",
    "# print(unique_nodes_list)\n",
    "unique_nodes = {n:i for i, n in enumerate(unique_nodes_list)}\n",
    "# print(unique_nodes)\n",
    "mask = Variable(torch.zeros(len(samp_neighs), len(unique_nodes)))\n",
    "column_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh]\n",
    "row_indices = [i for i in range(len(samp_neighs)) for j in range(len(samp_neighs[i]))]\n",
    "mask[row_indices, column_indices] = 1\n",
    "num_neigh = mask.sum(1, keepdims=True)\n",
    "mask = mask.div(num_neigh) # 对每个节点的影响权权重(此处为mean)，节点自身与采样节点的权重一致"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "embed_matrix = features(torch.LongTensor(unique_nodes_list)) # 34个节点，每个节点都有唯一的编码\n",
    "# print(embed_matrix)\n",
    "to_feats = mask.mm(embed_matrix)\n",
    "# print(mask.shape)\n",
    "# print(embed_matrix.shape)\n",
    "# print(to_feats.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([[-0.2784, -0.1315,  0.1929,  0.7867],\n        [-0.5944, -0.8150, -0.4203, -0.7069],\n        [-0.3423, -0.4274,  0.1675, -0.5433]], requires_grad=True)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = nn.Parameter(torch.FloatTensor(3, 2 if False else 4))\n",
    "init.xavier_uniform_(weight)\n",
    "# print(weight)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "self_feats = features(torch.LongTensor(nodes))\n",
    "# print(self_feats.shape)\n",
    "# print(features)\n",
    "# print(torch.LongTensor(nodes))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<map object at 0x0000019404BA7520>\n"
     ]
    }
   ],
   "source": [
    "a = \"asd qwert zxcv\"\n",
    "a = a.strip().split()\n",
    "print(map(float, a[1:-1]))\n",
    "c = np.zeros((3,3))\n",
    "# c[1,:]=map(float, a[1:-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}