{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter # 也可直接调用nn.Parameter\n",
    "from torch.nn.modules.module import Module # 也可直接调用nn.Module\n",
    "\n",
    "# 注意：torch.FloatTensor生成的元素数值非常接近0;torch.Long生成的元素数值非常大\n",
    "class GraphConvolution(Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        # self.weight2 = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None) #\n",
    "        self.reset_parameters() # 此处表示生成变量后(即上面的语句运行后),将会进行变量初始化(即执行该语句)\n",
    "\n",
    "    def reset_parameters(self): # 经测试,重写可覆盖\n",
    "        stdv = 1/math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        # self.weight2.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        # support = torch.mm(input, self.weight)    # 尝试：此行与下一行交换顺序\n",
    "        # output = torch.spmm(adj, support) # torch.spmm支持sparse在前,dense在后的矩阵乘法\n",
    "        support = torch.spmm(adj, input)\n",
    "        output = torch.mm(support, self.weight)\n",
    "        if self.bias is not None:         # adj是稀疏矩阵\n",
    "            return output +self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__+\"(\"+str(self.in_features)+\"->\"+str(self.out_features)+\")\"\n",
    "\n",
    "# class Readout(Module):\n",
    "#     def __init__(self, in_features):\n",
    "#         super(Readout, self).__init__()\n",
    "#         self.in_features = in_features\n",
    "#         self.weight = Parameter(torch.FloatTensor(in_features, in_features))\n",
    "#         self.reset_parameters()\n",
    "#\n",
    "#     def reset_parameters(self):\n",
    "#         stdv = 1/math.sqrt(self.weight.size(1))\n",
    "#         self.weight.data.uniform_(-stdv, stdv)\n",
    "#\n",
    "#     def forward(self, input):\n",
    "#         input = torch.sum(input, 0) # 此处不确定\n",
    "#         return torch.mm(input, self.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        # self.gc2 = GraphConvolution(nhid, nhid)\n",
    "        self.dropout = dropout\n",
    "        # self.rd = Readout(nhid)\n",
    "        # self.fc1 = nn.Linear(nhid, 1)\n",
    "        # nn.init.kaiming_normal_(self.fc1.weight)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "        # x = F.relu(self.gc2(x, adj))\n",
    "        # x = F.dropout(x, self.dropout, training=self.training)\n",
    "        # x = F.relu(self.rd(x))\n",
    "        # x = self.fc(x)\n",
    "        # return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{9, 3, 6}\n",
      "[3, 6, 9]\n",
      "{9: array([1., 0., 0.]), 3: array([0., 1., 0.]), 6: array([0., 0., 1.])}\n",
      "[[0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]]\n",
      "<map object at 0x000001EB2546CCD0>\n",
      "[array([0., 1., 0.]), array([1., 0., 0.]), array([0., 0., 1.])]\n"
     ]
    }
   ],
   "source": [
    "# 理解one-hot编码的过程：\n",
    "import numpy as np\n",
    "a = [3, 9, 6]\n",
    "b = set(a)\n",
    "print(b)\n",
    "c = sorted(list(set(a)))\n",
    "print(c)\n",
    "d = {t: np.identity(len(b))[i, :] for i, t in enumerate(b)}\n",
    "# d = {t: np.identity(len(b))[i, :] for i, t in enumerate(c)}\n",
    "print(d)\n",
    "e = np.array(list(map(d.get, a)), dtype=np.int32) # c.get为字典内置函数\n",
    "print(e)\n",
    "print(map(d.get, a))\n",
    "print(list(map(d.get, a)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['31336' '0' '0' ... '0' '0' 'Neural_Networks']\n",
      " ['1061127' '0' '0' ... '0' '0' 'Rule_Learning']\n",
      " ['1106406' '0' '0' ... '0' '0' 'Reinforcement_Learning']\n",
      " ...\n",
      " ['1128978' '0' '0' ... '0' '0' 'Genetic_Algorithms']\n",
      " ['117328' '0' '0' ... '0' '0' 'Case_Based']\n",
      " ['24043' '0' '0' ... '0' '0' 'Neural_Networks']]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(2708, 1433)\n"
     ]
    }
   ],
   "source": [
    "# 理解np.genfromtxt和sp.csr_matrix的处理：\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "idx_features_labels = np.genfromtxt(\"{}{}.content\".format(\"./cora/\", \"cora\"), dtype=np.dtype(str))\n",
    "print(idx_features_labels)\n",
    "mid_idx_features_labels = idx_features_labels[:, 1:-1]\n",
    "after_idx_features_labels = np.zeros(mid_idx_features_labels.shape)\n",
    "for i in range(mid_idx_features_labels.shape[0]):\n",
    "    for j in range(mid_idx_features_labels.shape[1]):\n",
    "        after_idx_features_labels[i][j] = int(mid_idx_features_labels[i][j])\n",
    "features = sp.csr_matrix(after_idx_features_labels, dtype=np.float32)\n",
    "print(after_idx_features_labels)\n",
    "# print(features)\n",
    "print(features.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "# 注意此处a中的元素为字符串，且直接在a上修改覆盖无法生效，故要创建另一个与a的shape一致的变量\n",
    "a = idx_features_labels[1:5, 1:5]\n",
    "b = np.zeros(a.shape)\n",
    "for i in range(a.shape[0]):\n",
    "    for j in range(a.shape[1]):\n",
    "        # a[i][j] = int(a[i][j])\n",
    "        b[i][j] = int(a[i][j])\n",
    "\n",
    "# print(a[0][0]-1)\n",
    "print(b[0][0]-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c:np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def load_data(path=\"./cora/\", dataset=\"cora\"):\n",
    "    print(\"Loading {} dataset...\".format(dataset))\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset), dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j:i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset), dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
    "    adj = adj + adj.T.multiply(adj.T>adj) - adj.multiply(adj.T>adj) # 最后减的那一项目的是去除负边\n",
    "\n",
    "    features = normalize(features)\n",
    "    # adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "    adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "def normalize(mx):\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv) # 此处得到一个对角矩阵\n",
    "    mx = r_mat_inv.dot(mx) # 注意.dot为矩阵乘法,不是对应元素相乘\n",
    "    return mx\n",
    "\n",
    "def normalize_adj(mx):\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "    mid = np.dot(r_mat_inv_sqrt, mx)\n",
    "    return np.dot(mid, r_mat_inv_sqrt)\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels).reshape(labels.shape)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = sparse_mx.shape\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  3  2]\n",
      " [ 0  4  5]\n",
      " [ 1  3  6]\n",
      " [ 1  5  7]\n",
      " [ 2  4 10]]\n",
      "--------------------------------------------------\n",
      "  (0, 3)\t2.0\n",
      "  (0, 4)\t5.0\n",
      "  (1, 3)\t6.0\n",
      "  (1, 5)\t7.0\n",
      "  (2, 4)\t10.0\n",
      "--------------------------------------------------\n",
      "[[ 0.  0.  0.  2.  5.  0.  0.]\n",
      " [ 0.  0.  0.  6.  0.  7.  0.]\n",
      " [ 0.  0.  0.  0. 10.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# 理解.todense()\n",
    "a = np.array([[0,3,2],[0,4,5],[1,3,6],[1,5,7],[2,4,10]])\n",
    "b = sp.coo_matrix(arg1=(a[:, 2], (a[:, 0], a[:, 1])), shape=(7,7), dtype=np.float32)\n",
    "c = b.todense()\n",
    "print(a)\n",
    "print(\"-\"*50)\n",
    "print(b)\n",
    "print(\"-\"*50)\n",
    "print(c)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " ...\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]]\n",
      "[   0    1    2 ... 2705 2706 2707]\n",
      "[2 0 6 ... 4 3 2]\n",
      "[0 0 0 0 0 1 1 1 1 1]\n",
      "[0 1 3 5 8 0 1 3 5 8]\n",
      "tensor([[ 1.,  4.,  0., -2.,  0.,  3.,  0.,  0., -5.],\n",
      "        [ 1.,  4.,  0.,  2.,  0.,  3.,  0.,  0.,  5.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 理解np.where\n",
    "labels = encode_onehot(idx_features_labels[:, -1])\n",
    "print(labels)\n",
    "print(np.where(labels)[0])\n",
    "print(np.where(labels)[1])\n",
    "a = [[1, 4, 0, -2, 0, 3, 0, 0, -5],[1, 4, 0, 2, 0, 3, 0, 0, 5]]\n",
    "print(np.where(a)[0])\n",
    "print(np.where(a)[1])\n",
    "print(torch.tensor(a).double())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4187, 0.6349, 0.2859, 0.0057, 0.0730],\n",
      "        [0.7526, 0.4721, 0.8668, 0.9557, 0.6298],\n",
      "        [0.1914, 0.0729, 0.1411, 0.7865, 0.2367]])\n",
      "torch.return_types.max(\n",
      "values=tensor([0.6349, 0.9557, 0.7865]),\n",
      "indices=tensor([1, 3, 3]))\n",
      "tensor([1, 3, 3], dtype=torch.int32)\n",
      "int32\n"
     ]
    }
   ],
   "source": [
    "# 理解.max(1)[1], .type_as()\n",
    "b = torch.rand(3,5)\n",
    "print(b)\n",
    "print(b.max(1))\n",
    "print(b.max(1)[1].type_as(torch.tensor(labels)))\n",
    "print(labels.dtype)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "# 理解.astype(np.float32)\n",
    "c = np.array(b).astype(np.float32) # 只有np.array才能用astype\n",
    "print(c.dtype)\n",
    "print(b.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 5.]\n",
      " [0. 2. 0.]\n",
      " [0. 0. 3.]]\n",
      "[6. 2. 3.]\n",
      "[0.16666667 0.5        0.33333333]\n",
      "  (0, 0)\t0.16666666666666666\n",
      "  (1, 1)\t0.5\n",
      "  (2, 2)\t0.3333333333333333\n",
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "# 理解函数normalize的细节\n",
    "a = np.array([[1, 0, 5], [0, 2, 0], [0, 0, 3]], dtype=np.float64)\n",
    "print(a)\n",
    "b = np.array(a.sum(1))\n",
    "print(b)\n",
    "b = np.power(b, -1).flatten()\n",
    "b[np.isinf(b)] = 0.\n",
    "print(b)\n",
    "c = a.flatten()\n",
    "d = sp.diags(b)\n",
    "print(d)\n",
    "print(d.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     35    1033]\n",
      " [     35  103482]\n",
      " [     35  103515]\n",
      " ...\n",
      " [ 853118 1140289]\n",
      " [ 853155  853118]\n",
      " [ 954315 1155073]]\n",
      "--------------------------------------------------\n",
      "[[ 163  402]\n",
      " [ 163  659]\n",
      " [ 163 1696]\n",
      " ...\n",
      " [1887 2258]\n",
      " [1902 1887]\n",
      " [ 837 1686]]\n"
     ]
    }
   ],
   "source": [
    "# 理解sp.coo_matrix的处理：\n",
    "labels = encode_onehot(idx_features_labels[:, -1])\n",
    "idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "idx_map = {j: i for i, j in enumerate(idx)}\n",
    "edges_unordered = np.genfromtxt(\"{}{}.cites\".format(\"./cora/\", \"cora\"),dtype=np.int32)\n",
    "# print(edges_unordered.shape)\n",
    "# print(edges_unordered.flatten().shape)\n",
    "edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "# 为了格式能成功转换,此处需要调用.flatten()改变shape,最后再转回来\n",
    "print(edges_unordered)\n",
    "print(\"-\"*50)\n",
    "print(edges)\n",
    "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
    "# print(adj)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(dropout=0.5, epochs=200, fastmode=True, hidden=16, lr=0.01, no_cuda=False, seed=42, weight_decay=0.0005)\n",
      "Loading cora dataset...\n",
      "Epoch:0001 loss_train:2.0336 acc_train:0.0714 loss_val:2.0350 acc_val:0.0933 time:0.0480\n",
      "Epoch:0002 loss_train:2.0109 acc_train:0.0714 loss_val:2.0178 acc_val:0.1133 time:0.0440\n",
      "Epoch:0003 loss_train:1.9999 acc_train:0.0857 loss_val:2.0020 acc_val:0.1000 time:0.0420\n",
      "Epoch:0004 loss_train:1.9795 acc_train:0.0857 loss_val:1.9836 acc_val:0.1000 time:0.0400\n",
      "Epoch:0005 loss_train:1.9621 acc_train:0.1000 loss_val:1.9706 acc_val:0.1100 time:0.0400\n",
      "Epoch:0006 loss_train:1.9420 acc_train:0.1357 loss_val:1.9550 acc_val:0.1033 time:0.0370\n",
      "Epoch:0007 loss_train:1.9257 acc_train:0.1571 loss_val:1.9398 acc_val:0.1567 time:0.0390\n",
      "Epoch:0008 loss_train:1.9175 acc_train:0.1857 loss_val:1.9280 acc_val:0.2167 time:0.0406\n",
      "Epoch:0009 loss_train:1.8994 acc_train:0.2929 loss_val:1.9171 acc_val:0.2333 time:0.0370\n",
      "Epoch:0010 loss_train:1.8940 acc_train:0.2714 loss_val:1.9062 acc_val:0.2867 time:0.0380\n",
      "Epoch:0011 loss_train:1.8736 acc_train:0.2857 loss_val:1.8894 acc_val:0.2900 time:0.0380\n",
      "Epoch:0012 loss_train:1.8591 acc_train:0.3143 loss_val:1.8807 acc_val:0.3400 time:0.0409\n",
      "Epoch:0013 loss_train:1.8517 acc_train:0.3286 loss_val:1.8685 acc_val:0.3500 time:0.0386\n",
      "Epoch:0014 loss_train:1.8351 acc_train:0.3714 loss_val:1.8579 acc_val:0.3533 time:0.0370\n",
      "Epoch:0015 loss_train:1.8184 acc_train:0.3429 loss_val:1.8446 acc_val:0.3400 time:0.0390\n",
      "Epoch:0016 loss_train:1.8242 acc_train:0.3143 loss_val:1.8480 acc_val:0.3400 time:0.0370\n",
      "Epoch:0017 loss_train:1.7946 acc_train:0.3429 loss_val:1.8304 acc_val:0.3633 time:0.0360\n",
      "Epoch:0018 loss_train:1.7875 acc_train:0.3286 loss_val:1.8176 acc_val:0.3167 time:0.0390\n",
      "Epoch:0019 loss_train:1.7660 acc_train:0.3500 loss_val:1.8153 acc_val:0.3633 time:0.0376\n",
      "Epoch:0020 loss_train:1.7716 acc_train:0.3286 loss_val:1.8021 acc_val:0.3400 time:0.0370\n",
      "Epoch:0021 loss_train:1.7668 acc_train:0.3143 loss_val:1.8091 acc_val:0.3367 time:0.0370\n",
      "Epoch:0022 loss_train:1.7595 acc_train:0.3500 loss_val:1.7951 acc_val:0.3700 time:0.0380\n",
      "Epoch:0023 loss_train:1.7650 acc_train:0.3357 loss_val:1.8056 acc_val:0.3533 time:0.0370\n",
      "Epoch:0024 loss_train:1.7590 acc_train:0.3286 loss_val:1.7981 acc_val:0.3767 time:0.0410\n",
      "Epoch:0025 loss_train:1.7315 acc_train:0.3643 loss_val:1.7735 acc_val:0.3667 time:0.0400\n",
      "Epoch:0026 loss_train:1.7307 acc_train:0.3286 loss_val:1.7952 acc_val:0.3600 time:0.0380\n",
      "Epoch:0027 loss_train:1.7084 acc_train:0.3857 loss_val:1.7776 acc_val:0.3667 time:0.0380\n",
      "Epoch:0028 loss_train:1.6848 acc_train:0.3643 loss_val:1.7736 acc_val:0.3533 time:0.0360\n",
      "Epoch:0029 loss_train:1.7197 acc_train:0.3500 loss_val:1.7841 acc_val:0.3467 time:0.0400\n",
      "Epoch:0030 loss_train:1.6952 acc_train:0.3143 loss_val:1.7499 acc_val:0.3600 time:0.0410\n",
      "Epoch:0031 loss_train:1.6975 acc_train:0.3357 loss_val:1.7420 acc_val:0.3733 time:0.0400\n",
      "Epoch:0032 loss_train:1.6781 acc_train:0.3357 loss_val:1.7294 acc_val:0.3600 time:0.0390\n",
      "Epoch:0033 loss_train:1.6553 acc_train:0.3429 loss_val:1.7286 acc_val:0.3600 time:0.0400\n",
      "Epoch:0034 loss_train:1.6672 acc_train:0.3143 loss_val:1.7330 acc_val:0.3667 time:0.0416\n",
      "Epoch:0035 loss_train:1.6472 acc_train:0.3286 loss_val:1.7036 acc_val:0.3667 time:0.0410\n",
      "Epoch:0036 loss_train:1.6338 acc_train:0.3857 loss_val:1.7168 acc_val:0.3633 time:0.0380\n",
      "Epoch:0037 loss_train:1.6216 acc_train:0.3643 loss_val:1.7037 acc_val:0.3567 time:0.0410\n",
      "Epoch:0038 loss_train:1.6020 acc_train:0.3929 loss_val:1.7075 acc_val:0.3933 time:0.0360\n",
      "Epoch:0039 loss_train:1.6135 acc_train:0.4000 loss_val:1.6930 acc_val:0.3833 time:0.0370\n",
      "Epoch:0040 loss_train:1.5943 acc_train:0.4357 loss_val:1.6807 acc_val:0.4000 time:0.0400\n",
      "Epoch:0041 loss_train:1.5663 acc_train:0.4643 loss_val:1.6603 acc_val:0.4033 time:0.0400\n",
      "Epoch:0042 loss_train:1.5560 acc_train:0.4071 loss_val:1.6651 acc_val:0.4300 time:0.0380\n",
      "Epoch:0043 loss_train:1.5405 acc_train:0.5000 loss_val:1.6403 acc_val:0.4433 time:0.0390\n",
      "Epoch:0044 loss_train:1.5302 acc_train:0.5000 loss_val:1.6412 acc_val:0.4167 time:0.0416\n",
      "Epoch:0045 loss_train:1.5055 acc_train:0.5071 loss_val:1.6373 acc_val:0.4433 time:0.0440\n",
      "Epoch:0046 loss_train:1.5033 acc_train:0.5071 loss_val:1.6352 acc_val:0.4800 time:0.0430\n",
      "Epoch:0047 loss_train:1.5054 acc_train:0.5143 loss_val:1.6227 acc_val:0.4733 time:0.0430\n",
      "Epoch:0048 loss_train:1.4913 acc_train:0.5357 loss_val:1.6179 acc_val:0.4733 time:0.0440\n",
      "Epoch:0049 loss_train:1.4567 acc_train:0.5286 loss_val:1.5843 acc_val:0.4667 time:0.0400\n",
      "Epoch:0050 loss_train:1.4255 acc_train:0.6500 loss_val:1.5799 acc_val:0.4967 time:0.0360\n",
      "Epoch:0051 loss_train:1.4339 acc_train:0.5500 loss_val:1.5778 acc_val:0.5133 time:0.0380\n",
      "Epoch:0052 loss_train:1.4089 acc_train:0.6143 loss_val:1.5577 acc_val:0.5433 time:0.0416\n",
      "Epoch:0053 loss_train:1.3732 acc_train:0.6500 loss_val:1.5555 acc_val:0.4933 time:0.0390\n",
      "Epoch:0054 loss_train:1.3758 acc_train:0.6000 loss_val:1.5304 acc_val:0.5233 time:0.0390\n",
      "Epoch:0055 loss_train:1.3463 acc_train:0.6500 loss_val:1.5289 acc_val:0.5200 time:0.0390\n",
      "Epoch:0056 loss_train:1.3518 acc_train:0.6929 loss_val:1.5085 acc_val:0.5633 time:0.0410\n",
      "Epoch:0057 loss_train:1.3279 acc_train:0.6571 loss_val:1.5003 acc_val:0.5800 time:0.0370\n",
      "Epoch:0058 loss_train:1.3322 acc_train:0.6786 loss_val:1.4921 acc_val:0.5633 time:0.0390\n",
      "Epoch:0059 loss_train:1.2894 acc_train:0.7214 loss_val:1.4745 acc_val:0.5700 time:0.0430\n",
      "Epoch:0060 loss_train:1.2463 acc_train:0.7429 loss_val:1.4691 acc_val:0.5367 time:0.0440\n",
      "Epoch:0061 loss_train:1.2755 acc_train:0.7143 loss_val:1.4521 acc_val:0.5800 time:0.0430\n",
      "Epoch:0062 loss_train:1.2524 acc_train:0.6786 loss_val:1.4463 acc_val:0.6000 time:0.0460\n",
      "Epoch:0063 loss_train:1.2132 acc_train:0.7571 loss_val:1.4195 acc_val:0.6067 time:0.0430\n",
      "Epoch:0064 loss_train:1.2117 acc_train:0.7286 loss_val:1.4367 acc_val:0.5900 time:0.0390\n",
      "Epoch:0065 loss_train:1.2020 acc_train:0.7429 loss_val:1.3896 acc_val:0.6200 time:0.0390\n",
      "Epoch:0066 loss_train:1.1583 acc_train:0.7286 loss_val:1.4181 acc_val:0.5700 time:0.0400\n",
      "Epoch:0067 loss_train:1.1228 acc_train:0.7786 loss_val:1.3515 acc_val:0.6433 time:0.0380\n",
      "Epoch:0068 loss_train:1.1786 acc_train:0.7714 loss_val:1.3643 acc_val:0.6333 time:0.0420\n",
      "Epoch:0069 loss_train:1.1080 acc_train:0.7571 loss_val:1.3468 acc_val:0.6267 time:0.0376\n",
      "Epoch:0070 loss_train:1.0951 acc_train:0.7929 loss_val:1.3374 acc_val:0.6167 time:0.0400\n",
      "Epoch:0071 loss_train:1.0992 acc_train:0.7714 loss_val:1.3394 acc_val:0.6267 time:0.0360\n",
      "Epoch:0072 loss_train:1.0997 acc_train:0.7643 loss_val:1.3172 acc_val:0.6633 time:0.0420\n",
      "Epoch:0073 loss_train:1.0905 acc_train:0.7500 loss_val:1.3355 acc_val:0.6233 time:0.0410\n",
      "Epoch:0074 loss_train:1.0592 acc_train:0.8000 loss_val:1.2877 acc_val:0.6700 time:0.0380\n",
      "Epoch:0075 loss_train:1.0292 acc_train:0.7929 loss_val:1.2723 acc_val:0.6833 time:0.0440\n",
      "Epoch:0076 loss_train:1.0281 acc_train:0.8000 loss_val:1.2804 acc_val:0.6633 time:0.0420\n",
      "Epoch:0077 loss_train:1.0185 acc_train:0.7571 loss_val:1.2462 acc_val:0.6433 time:0.0450\n",
      "Epoch:0078 loss_train:0.9624 acc_train:0.7929 loss_val:1.2387 acc_val:0.6500 time:0.0420\n",
      "Epoch:0079 loss_train:0.9966 acc_train:0.7643 loss_val:1.2293 acc_val:0.6600 time:0.0430\n",
      "Epoch:0080 loss_train:0.9657 acc_train:0.8214 loss_val:1.2428 acc_val:0.6433 time:0.0400\n",
      "Epoch:0081 loss_train:0.9716 acc_train:0.7929 loss_val:1.2043 acc_val:0.6567 time:0.0400\n",
      "Epoch:0082 loss_train:0.9435 acc_train:0.7929 loss_val:1.2421 acc_val:0.6467 time:0.0380\n",
      "Epoch:0083 loss_train:0.9180 acc_train:0.8000 loss_val:1.2185 acc_val:0.6500 time:0.0356\n",
      "Epoch:0084 loss_train:0.8938 acc_train:0.8214 loss_val:1.2129 acc_val:0.6633 time:0.0380\n",
      "Epoch:0085 loss_train:0.9483 acc_train:0.7714 loss_val:1.2090 acc_val:0.6500 time:0.0420\n",
      "Epoch:0086 loss_train:0.8980 acc_train:0.8000 loss_val:1.2144 acc_val:0.6667 time:0.0424\n",
      "Epoch:0087 loss_train:0.9029 acc_train:0.8286 loss_val:1.1604 acc_val:0.7133 time:0.0430\n",
      "Epoch:0088 loss_train:0.8884 acc_train:0.8214 loss_val:1.1463 acc_val:0.6967 time:0.0420\n",
      "Epoch:0089 loss_train:0.8611 acc_train:0.8429 loss_val:1.1616 acc_val:0.6900 time:0.0410\n",
      "Epoch:0090 loss_train:0.8614 acc_train:0.8286 loss_val:1.1477 acc_val:0.6733 time:0.0380\n",
      "Epoch:0091 loss_train:0.8515 acc_train:0.8286 loss_val:1.1342 acc_val:0.6933 time:0.0370\n",
      "Epoch:0092 loss_train:0.8571 acc_train:0.8071 loss_val:1.1570 acc_val:0.6567 time:0.0360\n",
      "Epoch:0093 loss_train:0.8547 acc_train:0.8143 loss_val:1.0997 acc_val:0.7233 time:0.0375\n",
      "Epoch:0094 loss_train:0.8442 acc_train:0.8357 loss_val:1.1121 acc_val:0.7200 time:0.0400\n",
      "Epoch:0095 loss_train:0.8207 acc_train:0.8429 loss_val:1.0747 acc_val:0.7167 time:0.0400\n",
      "Epoch:0096 loss_train:0.8004 acc_train:0.8429 loss_val:1.0797 acc_val:0.7133 time:0.0400\n",
      "Epoch:0097 loss_train:0.8073 acc_train:0.8286 loss_val:1.0907 acc_val:0.7067 time:0.0400\n",
      "Epoch:0098 loss_train:0.7849 acc_train:0.8429 loss_val:1.1078 acc_val:0.7033 time:0.0370\n",
      "Epoch:0099 loss_train:0.8050 acc_train:0.8214 loss_val:1.0879 acc_val:0.7233 time:0.0370\n",
      "Epoch:0100 loss_train:0.7863 acc_train:0.8286 loss_val:1.0798 acc_val:0.7100 time:0.0360\n",
      "Epoch:0101 loss_train:0.7761 acc_train:0.8643 loss_val:1.0764 acc_val:0.7200 time:0.0430\n",
      "Epoch:0102 loss_train:0.7571 acc_train:0.8857 loss_val:1.0710 acc_val:0.7067 time:0.0410\n",
      "Epoch:0103 loss_train:0.7391 acc_train:0.8786 loss_val:1.0356 acc_val:0.7167 time:0.0400\n",
      "Epoch:0104 loss_train:0.7710 acc_train:0.8571 loss_val:1.0792 acc_val:0.7033 time:0.0420\n",
      "Epoch:0105 loss_train:0.7551 acc_train:0.8786 loss_val:1.1034 acc_val:0.6700 time:0.0380\n",
      "Epoch:0106 loss_train:0.7489 acc_train:0.8429 loss_val:1.0860 acc_val:0.7067 time:0.0410\n",
      "Epoch:0107 loss_train:0.7247 acc_train:0.8857 loss_val:1.0276 acc_val:0.6867 time:0.0410\n",
      "Epoch:0108 loss_train:0.7196 acc_train:0.8786 loss_val:1.0061 acc_val:0.7233 time:0.0366\n",
      "Epoch:0109 loss_train:0.6828 acc_train:0.8643 loss_val:0.9904 acc_val:0.7333 time:0.0380\n",
      "Epoch:0110 loss_train:0.7067 acc_train:0.8929 loss_val:1.0186 acc_val:0.7133 time:0.0430\n",
      "Epoch:0111 loss_train:0.7129 acc_train:0.9143 loss_val:1.0385 acc_val:0.6967 time:0.0420\n",
      "Epoch:0112 loss_train:0.6622 acc_train:0.9000 loss_val:1.0143 acc_val:0.7633 time:0.0370\n",
      "Epoch:0113 loss_train:0.6888 acc_train:0.9071 loss_val:1.0127 acc_val:0.7233 time:0.0370\n",
      "Epoch:0114 loss_train:0.6683 acc_train:0.9071 loss_val:1.0313 acc_val:0.7233 time:0.0400\n",
      "Epoch:0115 loss_train:0.6557 acc_train:0.9286 loss_val:1.0257 acc_val:0.7100 time:0.0380\n",
      "Epoch:0116 loss_train:0.6713 acc_train:0.8929 loss_val:1.0636 acc_val:0.7167 time:0.0380\n",
      "Epoch:0117 loss_train:0.6770 acc_train:0.9000 loss_val:1.0302 acc_val:0.7067 time:0.0390\n",
      "Epoch:0118 loss_train:0.6877 acc_train:0.8643 loss_val:1.0080 acc_val:0.7200 time:0.0380\n",
      "Epoch:0119 loss_train:0.6345 acc_train:0.9071 loss_val:0.9764 acc_val:0.7667 time:0.0382\n",
      "Epoch:0120 loss_train:0.6277 acc_train:0.8929 loss_val:0.9664 acc_val:0.7567 time:0.0400\n",
      "Epoch:0121 loss_train:0.6408 acc_train:0.9000 loss_val:0.9860 acc_val:0.7433 time:0.0380\n",
      "Epoch:0122 loss_train:0.5967 acc_train:0.9071 loss_val:0.9925 acc_val:0.7233 time:0.0370\n",
      "Epoch:0123 loss_train:0.6154 acc_train:0.9500 loss_val:0.9895 acc_val:0.7433 time:0.0400\n",
      "Epoch:0124 loss_train:0.5772 acc_train:0.9214 loss_val:0.9315 acc_val:0.7567 time:0.0400\n",
      "Epoch:0125 loss_train:0.5935 acc_train:0.9000 loss_val:0.9526 acc_val:0.7100 time:0.0410\n",
      "Epoch:0126 loss_train:0.6341 acc_train:0.9143 loss_val:0.9380 acc_val:0.7333 time:0.0390\n",
      "Epoch:0127 loss_train:0.5986 acc_train:0.9000 loss_val:0.9289 acc_val:0.7633 time:0.0393\n",
      "Epoch:0128 loss_train:0.5939 acc_train:0.9143 loss_val:0.9353 acc_val:0.7267 time:0.0380\n",
      "Epoch:0129 loss_train:0.5698 acc_train:0.9286 loss_val:0.9650 acc_val:0.7533 time:0.0380\n",
      "Epoch:0130 loss_train:0.5843 acc_train:0.9429 loss_val:0.9498 acc_val:0.7533 time:0.0370\n",
      "Epoch:0131 loss_train:0.5900 acc_train:0.9214 loss_val:0.9264 acc_val:0.7300 time:0.0420\n",
      "Epoch:0132 loss_train:0.6050 acc_train:0.8929 loss_val:0.9217 acc_val:0.7567 time:0.0390\n",
      "Epoch:0133 loss_train:0.5611 acc_train:0.8786 loss_val:0.9336 acc_val:0.7533 time:0.0396\n",
      "Epoch:0134 loss_train:0.5578 acc_train:0.9071 loss_val:0.9181 acc_val:0.7467 time:0.0400\n",
      "Epoch:0135 loss_train:0.5600 acc_train:0.9143 loss_val:0.9349 acc_val:0.7400 time:0.0390\n",
      "Epoch:0136 loss_train:0.5943 acc_train:0.9000 loss_val:0.9108 acc_val:0.7700 time:0.0500\n",
      "Epoch:0137 loss_train:0.5238 acc_train:0.9143 loss_val:0.9437 acc_val:0.7600 time:0.0390\n",
      "Epoch:0138 loss_train:0.6039 acc_train:0.8714 loss_val:0.9515 acc_val:0.7467 time:0.0370\n",
      "Epoch:0139 loss_train:0.5722 acc_train:0.9071 loss_val:0.9345 acc_val:0.7300 time:0.0380\n",
      "Epoch:0140 loss_train:0.5478 acc_train:0.9143 loss_val:0.9350 acc_val:0.7600 time:0.0379\n",
      "Epoch:0141 loss_train:0.5262 acc_train:0.9571 loss_val:0.9543 acc_val:0.7367 time:0.0420\n",
      "Epoch:0142 loss_train:0.5406 acc_train:0.9286 loss_val:0.9335 acc_val:0.7467 time:0.0400\n",
      "Epoch:0143 loss_train:0.5416 acc_train:0.9286 loss_val:0.9053 acc_val:0.7633 time:0.0390\n",
      "Epoch:0144 loss_train:0.5169 acc_train:0.9071 loss_val:0.8879 acc_val:0.7500 time:0.0386\n",
      "Epoch:0145 loss_train:0.5079 acc_train:0.9357 loss_val:0.8846 acc_val:0.7600 time:0.0390\n",
      "Epoch:0146 loss_train:0.5046 acc_train:0.9429 loss_val:0.9135 acc_val:0.7500 time:0.0390\n",
      "Epoch:0147 loss_train:0.5282 acc_train:0.9143 loss_val:0.8765 acc_val:0.7633 time:0.0400\n",
      "Epoch:0148 loss_train:0.5278 acc_train:0.9286 loss_val:0.9104 acc_val:0.7633 time:0.0390\n",
      "Epoch:0149 loss_train:0.4928 acc_train:0.9214 loss_val:0.8792 acc_val:0.7600 time:0.0400\n",
      "Epoch:0150 loss_train:0.4901 acc_train:0.9571 loss_val:0.8823 acc_val:0.7700 time:0.0400\n",
      "Epoch:0151 loss_train:0.4709 acc_train:0.9500 loss_val:0.9082 acc_val:0.7567 time:0.0360\n",
      "Epoch:0152 loss_train:0.4869 acc_train:0.9357 loss_val:0.8669 acc_val:0.7600 time:0.0400\n",
      "Epoch:0153 loss_train:0.5117 acc_train:0.9357 loss_val:0.8797 acc_val:0.7667 time:0.0400\n",
      "Epoch:0154 loss_train:0.5124 acc_train:0.9214 loss_val:0.9118 acc_val:0.7333 time:0.0400\n",
      "Epoch:0155 loss_train:0.5003 acc_train:0.9071 loss_val:0.9133 acc_val:0.7500 time:0.0450\n",
      "Epoch:0156 loss_train:0.4654 acc_train:0.9429 loss_val:0.8420 acc_val:0.7867 time:0.0440\n",
      "Epoch:0157 loss_train:0.5351 acc_train:0.9071 loss_val:0.9145 acc_val:0.7300 time:0.0450\n",
      "Epoch:0158 loss_train:0.5049 acc_train:0.9214 loss_val:0.8545 acc_val:0.7700 time:0.0426\n",
      "Epoch:0159 loss_train:0.4716 acc_train:0.9357 loss_val:0.8307 acc_val:0.7800 time:0.0420\n",
      "Epoch:0160 loss_train:0.5239 acc_train:0.9071 loss_val:0.8717 acc_val:0.7667 time:0.0440\n",
      "Epoch:0161 loss_train:0.4869 acc_train:0.9429 loss_val:0.9068 acc_val:0.7367 time:0.0460\n",
      "Epoch:0162 loss_train:0.4819 acc_train:0.9286 loss_val:0.8733 acc_val:0.7867 time:0.0450\n",
      "Epoch:0163 loss_train:0.4745 acc_train:0.9500 loss_val:0.8167 acc_val:0.7833 time:0.0530\n",
      "Epoch:0164 loss_train:0.5131 acc_train:0.9000 loss_val:0.8506 acc_val:0.7933 time:0.0500\n",
      "Epoch:0165 loss_train:0.4556 acc_train:0.9286 loss_val:0.8251 acc_val:0.7900 time:0.0490\n",
      "Epoch:0166 loss_train:0.4880 acc_train:0.9214 loss_val:0.8521 acc_val:0.7533 time:0.0577\n",
      "Epoch:0167 loss_train:0.4823 acc_train:0.9286 loss_val:0.8254 acc_val:0.7800 time:0.0600\n",
      "Epoch:0168 loss_train:0.4405 acc_train:0.9429 loss_val:0.8408 acc_val:0.7933 time:0.0500\n",
      "Epoch:0169 loss_train:0.4471 acc_train:0.9429 loss_val:0.8486 acc_val:0.7433 time:0.0490\n",
      "Epoch:0170 loss_train:0.4504 acc_train:0.9500 loss_val:0.9105 acc_val:0.7667 time:0.0480\n",
      "Epoch:0171 loss_train:0.4787 acc_train:0.9214 loss_val:0.8475 acc_val:0.7533 time:0.0510\n",
      "Epoch:0172 loss_train:0.4327 acc_train:0.9500 loss_val:0.8551 acc_val:0.7600 time:0.0500\n",
      "Epoch:0173 loss_train:0.4282 acc_train:0.9429 loss_val:0.8507 acc_val:0.7633 time:0.0530\n",
      "Epoch:0174 loss_train:0.4764 acc_train:0.9429 loss_val:0.8482 acc_val:0.7467 time:0.0490\n",
      "Epoch:0175 loss_train:0.4272 acc_train:0.9357 loss_val:0.8572 acc_val:0.7600 time:0.0470\n",
      "Epoch:0176 loss_train:0.4736 acc_train:0.9357 loss_val:0.8530 acc_val:0.7767 time:0.0410\n",
      "Epoch:0177 loss_train:0.4062 acc_train:0.9571 loss_val:0.8641 acc_val:0.7433 time:0.0440\n",
      "Epoch:0178 loss_train:0.4379 acc_train:0.9214 loss_val:0.8562 acc_val:0.7400 time:0.0410\n",
      "Epoch:0179 loss_train:0.4562 acc_train:0.9143 loss_val:0.8530 acc_val:0.7533 time:0.0501\n",
      "Epoch:0180 loss_train:0.4410 acc_train:0.9571 loss_val:0.8171 acc_val:0.7667 time:0.0500\n",
      "Epoch:0181 loss_train:0.4350 acc_train:0.9357 loss_val:0.8083 acc_val:0.7800 time:0.0490\n",
      "Epoch:0182 loss_train:0.4333 acc_train:0.9214 loss_val:0.8587 acc_val:0.7533 time:0.0480\n",
      "Epoch:0183 loss_train:0.4515 acc_train:0.9429 loss_val:0.8354 acc_val:0.7467 time:0.0510\n",
      "Epoch:0184 loss_train:0.3847 acc_train:0.9571 loss_val:0.8123 acc_val:0.7633 time:0.0440\n",
      "Epoch:0185 loss_train:0.3686 acc_train:0.9429 loss_val:0.8288 acc_val:0.7633 time:0.0500\n",
      "Epoch:0186 loss_train:0.4059 acc_train:0.9357 loss_val:0.8050 acc_val:0.7900 time:0.0420\n",
      "Epoch:0187 loss_train:0.4288 acc_train:0.9571 loss_val:0.8450 acc_val:0.7367 time:0.0426\n",
      "Epoch:0188 loss_train:0.4424 acc_train:0.9357 loss_val:0.8041 acc_val:0.7433 time:0.0460\n",
      "Epoch:0189 loss_train:0.4041 acc_train:0.9286 loss_val:0.8460 acc_val:0.7767 time:0.0450\n",
      "Epoch:0190 loss_train:0.3983 acc_train:0.9571 loss_val:0.8347 acc_val:0.7700 time:0.0460\n",
      "Epoch:0191 loss_train:0.4369 acc_train:0.9286 loss_val:0.8165 acc_val:0.7933 time:0.0480\n",
      "Epoch:0192 loss_train:0.3941 acc_train:0.9286 loss_val:0.8289 acc_val:0.7933 time:0.0439\n",
      "Epoch:0193 loss_train:0.4276 acc_train:0.9357 loss_val:0.8028 acc_val:0.7833 time:0.0450\n",
      "Epoch:0194 loss_train:0.4188 acc_train:0.9643 loss_val:0.8487 acc_val:0.7433 time:0.0420\n",
      "Epoch:0195 loss_train:0.4524 acc_train:0.9286 loss_val:0.8391 acc_val:0.7700 time:0.0455\n",
      "Epoch:0196 loss_train:0.4322 acc_train:0.9429 loss_val:0.8196 acc_val:0.7400 time:0.0430\n",
      "Epoch:0197 loss_train:0.4191 acc_train:0.9500 loss_val:0.8248 acc_val:0.7733 time:0.0460\n",
      "Epoch:0198 loss_train:0.4244 acc_train:0.9357 loss_val:0.8392 acc_val:0.7433 time:0.0430\n",
      "Epoch:0199 loss_train:0.3780 acc_train:0.9429 loss_val:0.8296 acc_val:0.7600 time:0.0480\n",
      "Epoch:0200 loss_train:0.3834 acc_train:0.9714 loss_val:0.8223 acc_val:0.7533 time:0.0426\n",
      "Total time:8.3262s\n",
      "Test set results: loss=0.7775 accuracy=0.8110\n"
     ]
    }
   ],
   "source": [
    "# from __future__ import division\n",
    "# from __future__ import print_function\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--no-cuda\", action=\"store_true\", default=False, help=\"Disables CUDA training.\")\n",
    "parser.add_argument(\"--fastmode\", action=\"store_true\", default=True, help=\"Validate during training pass.\")\n",
    "parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed.\")\n",
    "parser.add_argument(\"--epochs\", type=int, default=200, help=\"Number of epochs to train.\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.01, help=\"Initial learning rate.\")\n",
    "parser.add_argument(\"--weight_decay\", type=float, default=5e-4, help=\"Weight decay(L2 loss on parameters).\")\n",
    "parser.add_argument(\"--hidden\", type=int, default=16, help=\"Number of hidden units\")\n",
    "parser.add_argument(\"--dropout\", type=float, default=0.5, help=\"Dropout rate (1 - keep probability).\")\n",
    "# args = parser.parse_args()\n",
    "args =parser.parse_known_args()[0]\n",
    "# print(parser)\n",
    "print(args)\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "np.random.seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
    "model = GCN(nfeat=features.shape[1], nhid=args.hidden, nclass=labels.max().item()+1, dropout=args.dropout)\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not args.fastmode:\n",
    "        model.eval() # 注意：dropout会影响前向传播,从而影响预测结果\n",
    "        output = model(features, adj)\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print(\"Epoch:{:04d}\".format(epoch+1), \"loss_train:{:.4f}\".format(loss_train.item()), \"acc_train:{:.4f}\".format(acc_train.item()), \"loss_val:{:.4f}\".format(loss_val.item()), \"acc_val:{:.4f}\".format(acc_val.item()), \"time:{:.4f}\".format(time.time()-t))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\", \"loss={:.4f}\".format(loss_test.item()), \"accuracy={:.4f}\".format(acc_test.item()))\n",
    "\n",
    "t_total = time.time()\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch)\n",
    "print(\"Total time:{:.4f}s\".format(time.time()-t_total))\n",
    "\n",
    "test()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}