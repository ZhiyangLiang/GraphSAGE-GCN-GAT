{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1))) # 尝试：\"1\"应该换为一个超参数t\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        Wh = torch.mm(h, self.W) # h.shape:(N, in_features), Wh.shape:(N, out_features)\n",
    "        e = self._prepare_attentional_mechanism_input(Wh)\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj>0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, Wh)\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "    def _prepare_attentional_mechanism_input(self, Wh):\n",
    "        # Wh.shape (N, out_feature)\n",
    "        # self.a.shape (2 * out_feature, 1)\n",
    "        # Wh1&2.shape (N, 1)\n",
    "        # e.shape (N, N)\n",
    "\n",
    "        Wh1 = torch.matmul(Wh, self.a[:self.out_features, :])\n",
    "        Wh2 = torch.matmul(Wh, self.a[self.out_features:, :])\n",
    "        # a[:self.out_features, :]用于被处理的节点i\n",
    "        # a[self.out_features:, :]用于节点i邻域内的节点j\n",
    "        e = Wh1 * Wh2.T #broadcast add 注：源代码用的是\"+\" 尝试：对比二者效果\n",
    "        # e为N个节点中任意两个节点之间的相关度组成的矩阵(N*N)\n",
    "        return self.leakyrelu(e)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__+\"(\"+str(self.in_features)+\"->\"+str(self.out_features)+\")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module(\"attention_{}\".format(i), attention)\n",
    "        self.out_att = GraphAttentionLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)\n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training = self.training)\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1) # 每个attention的输出维度为8, 8个attention拼接即得64维\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_att(x, adj)) # out_att的输入维度为64, 输出维度为7, 即种类数\n",
    "        return F.log_softmax(x, dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "def encode_onehot(labels):\n",
    "    classes = sorted(list(set(labels)))\n",
    "    classes_dict = {c:np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def load_data(path=\"./cora/\", dataset=\"cora\"):\n",
    "    print(\"Loading {} dataset...\".format(dataset))\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset), dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j:i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset), dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "    features = normalize_features(features)\n",
    "    adj = normalize_adj(adj+sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    # print(idx_train.dtype)\n",
    "    idx_train = torch.LongTensor(idx_train) # 此行与下面两行未注释的语句可省略\n",
    "    # print(idx_train.dtype)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "def normalize_adj(mx):\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "    # return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
    "    mid = np.dot(r_mat_inv_sqrt, mx)\n",
    "    return np.dot(mid, r_mat_inv_sqrt)\n",
    "\n",
    "def normalize_features(mx):\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    return mx\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels).reshape(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# # 理解.transpose()\n",
    "# a = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "# print(a)\n",
    "# print(a.transpose())\n",
    "# # 理解自定义的accuracy函数\n",
    "# b = np.array([[1, 2, 2], [4, 5, 5]])\n",
    "# c = torch.tensor(a)\n",
    "# d = torch.tensor(b)\n",
    "# print(c.eq(d))\n",
    "# print(c.eq(d).sum())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--no_cuda\", action=\"store_true\", default=False, help=\"Disables CUDA training.\")\n",
    "parser.add_argument(\"--fastmode\", action=\"store_true\", default=False, help=\"Validate during training pass.\")\n",
    "parser.add_argument(\"--sparse\", action=\"store_true\", default=False, help=\"GAT with sparse version or not.\")\n",
    "parser.add_argument(\"--seed\", type=int, default=72, help=\"Random seed.\")\n",
    "parser.add_argument(\"--epochs\", type=int, default=100, help=\"Number of epochs to train.\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.005, help=\"Initial learning rate.\")\n",
    "parser.add_argument(\"--weight_decay\", type=float, default=5e-4, help=\"Weight dacay (L2 loss on parameters).\")\n",
    "parser.add_argument(\"--hidden\", type=int, default=8, help=\"Number of hidden units.\")\n",
    "parser.add_argument(\"--nb_heads\", type=int, default=8, help=\"Number of head attentions.\")\n",
    "parser.add_argument(\"--dropout\", type=float, default=0.6, help=\"Dropout rate (1 - keep probability).\")\n",
    "parser.add_argument(\"--alpha\", type=float, default=0.2, help=\"Alpha for the leaky_relu.\")\n",
    "parser.add_argument(\"--patience\", type=int, default=100, help=\"Patience\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args =parser.parse_known_args()[0]\n",
    "# print(args)\n",
    "# print(parser)\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
    "model = GAT(nfeat=features.shape[1], nhid=args.hidden, nclass=int(labels.max())+1, dropout=args.dropout, nheads=args.nb_heads, alpha=args.alpha)\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "features, adj, labels = Variable(features), Variable(adj), Variable(labels)\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    if not args.fastmode:\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.data.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.data.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.data.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.data.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "    return loss_val.data.item()\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.data.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.data.item()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 2.1246 acc_train: 0.0643 loss_val: 1.8932 acc_val: 0.2467 time: 4.5609s\n",
      "Epoch: 0002 loss_train: 1.9298 acc_train: 0.2214 loss_val: 1.7696 acc_val: 0.4967 time: 4.6212s\n",
      "Epoch: 0003 loss_train: 1.9016 acc_train: 0.2714 loss_val: 1.6614 acc_val: 0.5400 time: 4.6821s\n",
      "Epoch: 0004 loss_train: 1.7966 acc_train: 0.4000 loss_val: 1.5664 acc_val: 0.5933 time: 4.6584s\n",
      "Epoch: 0005 loss_train: 1.5643 acc_train: 0.4857 loss_val: 1.4833 acc_val: 0.6200 time: 4.6320s\n",
      "Epoch: 0006 loss_train: 1.5821 acc_train: 0.4643 loss_val: 1.4089 acc_val: 0.6733 time: 4.6671s\n",
      "Epoch: 0007 loss_train: 1.4673 acc_train: 0.4786 loss_val: 1.3407 acc_val: 0.6900 time: 4.6748s\n",
      "Epoch: 0008 loss_train: 1.5597 acc_train: 0.5071 loss_val: 1.2787 acc_val: 0.7067 time: 4.6611s\n",
      "Epoch: 0009 loss_train: 1.4175 acc_train: 0.5786 loss_val: 1.2217 acc_val: 0.7167 time: 4.7081s\n",
      "Epoch: 0010 loss_train: 1.3469 acc_train: 0.5500 loss_val: 1.1692 acc_val: 0.7367 time: 4.6502s\n",
      "Epoch: 0011 loss_train: 1.3209 acc_train: 0.5929 loss_val: 1.1224 acc_val: 0.7433 time: 4.7432s\n",
      "Epoch: 0012 loss_train: 1.2019 acc_train: 0.6571 loss_val: 1.0797 acc_val: 0.7600 time: 4.7195s\n",
      "Epoch: 0013 loss_train: 1.1559 acc_train: 0.7214 loss_val: 1.0394 acc_val: 0.7833 time: 4.7618s\n",
      "Epoch: 0014 loss_train: 1.0626 acc_train: 0.7000 loss_val: 1.0028 acc_val: 0.7933 time: 4.6690s\n",
      "Epoch: 0015 loss_train: 1.1826 acc_train: 0.6571 loss_val: 0.9703 acc_val: 0.7967 time: 4.7166s\n",
      "Epoch: 0016 loss_train: 1.1634 acc_train: 0.6929 loss_val: 0.9413 acc_val: 0.8033 time: 4.9060s\n",
      "Epoch: 0017 loss_train: 1.1246 acc_train: 0.6571 loss_val: 0.9152 acc_val: 0.8067 time: 5.1672s\n",
      "Epoch: 0018 loss_train: 0.9867 acc_train: 0.6857 loss_val: 0.8915 acc_val: 0.8133 time: 4.8926s\n",
      "Epoch: 0019 loss_train: 0.9852 acc_train: 0.7143 loss_val: 0.8706 acc_val: 0.8233 time: 4.9789s\n",
      "Epoch: 0020 loss_train: 1.0302 acc_train: 0.7000 loss_val: 0.8520 acc_val: 0.8233 time: 5.2155s\n",
      "Epoch: 0021 loss_train: 0.9660 acc_train: 0.7357 loss_val: 0.8360 acc_val: 0.8333 time: 5.1814s\n",
      "Epoch: 0022 loss_train: 1.0540 acc_train: 0.6643 loss_val: 0.8220 acc_val: 0.8333 time: 5.2327s\n",
      "Epoch: 0023 loss_train: 1.0964 acc_train: 0.6857 loss_val: 0.8106 acc_val: 0.8300 time: 5.0665s\n",
      "Epoch: 0024 loss_train: 1.0531 acc_train: 0.7071 loss_val: 0.8009 acc_val: 0.8400 time: 4.7941s\n",
      "Epoch: 0025 loss_train: 1.0253 acc_train: 0.7714 loss_val: 0.7917 acc_val: 0.8433 time: 4.8222s\n",
      "Epoch: 0026 loss_train: 0.8210 acc_train: 0.7786 loss_val: 0.7830 acc_val: 0.8400 time: 5.0373s\n",
      "Epoch: 0027 loss_train: 0.9763 acc_train: 0.7286 loss_val: 0.7754 acc_val: 0.8467 time: 4.8693s\n",
      "Epoch: 0028 loss_train: 0.8443 acc_train: 0.7714 loss_val: 0.7684 acc_val: 0.8433 time: 5.0582s\n",
      "Epoch: 0029 loss_train: 0.9558 acc_train: 0.7071 loss_val: 0.7621 acc_val: 0.8433 time: 5.1634s\n",
      "Epoch: 0030 loss_train: 0.8989 acc_train: 0.7643 loss_val: 0.7558 acc_val: 0.8400 time: 4.9330s\n",
      "Epoch: 0031 loss_train: 0.8646 acc_train: 0.7929 loss_val: 0.7488 acc_val: 0.8400 time: 5.1002s\n",
      "Epoch: 0032 loss_train: 0.7803 acc_train: 0.7857 loss_val: 0.7415 acc_val: 0.8433 time: 4.8611s\n",
      "Epoch: 0033 loss_train: 0.9270 acc_train: 0.7500 loss_val: 0.7353 acc_val: 0.8400 time: 4.7739s\n",
      "Epoch: 0034 loss_train: 0.8723 acc_train: 0.7286 loss_val: 0.7300 acc_val: 0.8400 time: 4.8129s\n",
      "Epoch: 0035 loss_train: 1.0032 acc_train: 0.6500 loss_val: 0.7249 acc_val: 0.8367 time: 4.7817s\n",
      "Epoch: 0036 loss_train: 0.9407 acc_train: 0.6929 loss_val: 0.7201 acc_val: 0.8333 time: 4.8348s\n",
      "Epoch: 0037 loss_train: 0.8278 acc_train: 0.7643 loss_val: 0.7152 acc_val: 0.8333 time: 4.8481s\n",
      "Epoch: 0038 loss_train: 0.8687 acc_train: 0.7357 loss_val: 0.7110 acc_val: 0.8367 time: 4.8006s\n",
      "Epoch: 0039 loss_train: 0.7959 acc_train: 0.7857 loss_val: 0.7071 acc_val: 0.8367 time: 4.7095s\n",
      "Epoch: 0040 loss_train: 0.7399 acc_train: 0.8071 loss_val: 0.7027 acc_val: 0.8367 time: 4.7920s\n",
      "Epoch: 0041 loss_train: 0.8496 acc_train: 0.7714 loss_val: 0.6987 acc_val: 0.8333 time: 4.7822s\n",
      "Epoch: 0042 loss_train: 0.8806 acc_train: 0.7500 loss_val: 0.6947 acc_val: 0.8333 time: 4.8861s\n",
      "Epoch: 0043 loss_train: 0.7622 acc_train: 0.7643 loss_val: 0.6909 acc_val: 0.8300 time: 4.8482s\n",
      "Epoch: 0044 loss_train: 0.7476 acc_train: 0.7786 loss_val: 0.6865 acc_val: 0.8300 time: 4.9936s\n",
      "Epoch: 0045 loss_train: 0.7668 acc_train: 0.7714 loss_val: 0.6824 acc_val: 0.8300 time: 4.8895s\n",
      "Epoch: 0046 loss_train: 0.7335 acc_train: 0.7857 loss_val: 0.6781 acc_val: 0.8300 time: 4.8956s\n",
      "Epoch: 0047 loss_train: 0.7357 acc_train: 0.8000 loss_val: 0.6730 acc_val: 0.8333 time: 4.8131s\n",
      "Epoch: 0048 loss_train: 0.6531 acc_train: 0.8214 loss_val: 0.6684 acc_val: 0.8367 time: 4.7980s\n",
      "Epoch: 0049 loss_train: 0.9226 acc_train: 0.7500 loss_val: 0.6646 acc_val: 0.8367 time: 4.7479s\n",
      "Epoch: 0050 loss_train: 0.8141 acc_train: 0.7857 loss_val: 0.6619 acc_val: 0.8367 time: 4.8289s\n",
      "Epoch: 0051 loss_train: 0.7502 acc_train: 0.7929 loss_val: 0.6594 acc_val: 0.8367 time: 4.8417s\n",
      "Epoch: 0052 loss_train: 0.7793 acc_train: 0.7786 loss_val: 0.6573 acc_val: 0.8400 time: 4.7267s\n",
      "Epoch: 0053 loss_train: 0.6346 acc_train: 0.8500 loss_val: 0.6550 acc_val: 0.8400 time: 4.8935s\n",
      "Epoch: 0054 loss_train: 0.6138 acc_train: 0.8071 loss_val: 0.6519 acc_val: 0.8400 time: 4.9438s\n",
      "Epoch: 0055 loss_train: 0.7596 acc_train: 0.7786 loss_val: 0.6489 acc_val: 0.8400 time: 4.8452s\n",
      "Epoch: 0056 loss_train: 0.6713 acc_train: 0.8000 loss_val: 0.6459 acc_val: 0.8433 time: 4.7738s\n",
      "Epoch: 0057 loss_train: 0.7930 acc_train: 0.8214 loss_val: 0.6429 acc_val: 0.8433 time: 4.7448s\n",
      "Epoch: 0058 loss_train: 0.6925 acc_train: 0.8286 loss_val: 0.6404 acc_val: 0.8433 time: 4.7920s\n",
      "Epoch: 0059 loss_train: 0.7716 acc_train: 0.7714 loss_val: 0.6377 acc_val: 0.8433 time: 4.7945s\n",
      "Epoch: 0060 loss_train: 0.6356 acc_train: 0.8071 loss_val: 0.6352 acc_val: 0.8433 time: 4.7766s\n",
      "Epoch: 0061 loss_train: 0.7043 acc_train: 0.8214 loss_val: 0.6326 acc_val: 0.8433 time: 4.8105s\n",
      "Epoch: 0062 loss_train: 0.8293 acc_train: 0.7786 loss_val: 0.6304 acc_val: 0.8433 time: 4.8088s\n",
      "Epoch: 0063 loss_train: 0.6543 acc_train: 0.7857 loss_val: 0.6290 acc_val: 0.8433 time: 4.8103s\n",
      "Epoch: 0064 loss_train: 0.6243 acc_train: 0.7929 loss_val: 0.6280 acc_val: 0.8433 time: 4.7822s\n",
      "Epoch: 0065 loss_train: 0.6612 acc_train: 0.7714 loss_val: 0.6267 acc_val: 0.8400 time: 4.7108s\n",
      "Epoch: 0066 loss_train: 0.6387 acc_train: 0.8286 loss_val: 0.6256 acc_val: 0.8433 time: 4.7339s\n",
      "Epoch: 0067 loss_train: 0.7053 acc_train: 0.7786 loss_val: 0.6250 acc_val: 0.8400 time: 4.8104s\n",
      "Epoch: 0068 loss_train: 0.7041 acc_train: 0.8143 loss_val: 0.6242 acc_val: 0.8400 time: 4.7715s\n",
      "Epoch: 0069 loss_train: 0.7316 acc_train: 0.7857 loss_val: 0.6232 acc_val: 0.8400 time: 4.7989s\n",
      "Epoch: 0070 loss_train: 0.7644 acc_train: 0.7857 loss_val: 0.6219 acc_val: 0.8400 time: 4.7657s\n",
      "Epoch: 0071 loss_train: 0.6835 acc_train: 0.7929 loss_val: 0.6208 acc_val: 0.8400 time: 4.8021s\n",
      "Epoch: 0072 loss_train: 0.6983 acc_train: 0.7714 loss_val: 0.6198 acc_val: 0.8400 time: 4.8606s\n",
      "Epoch: 0073 loss_train: 0.6259 acc_train: 0.7857 loss_val: 0.6188 acc_val: 0.8400 time: 4.8196s\n",
      "Epoch: 0074 loss_train: 0.6464 acc_train: 0.7714 loss_val: 0.6178 acc_val: 0.8400 time: 4.8025s\n",
      "Epoch: 0075 loss_train: 0.6447 acc_train: 0.7929 loss_val: 0.6164 acc_val: 0.8400 time: 4.8144s\n",
      "Epoch: 0076 loss_train: 0.6507 acc_train: 0.8143 loss_val: 0.6147 acc_val: 0.8400 time: 4.7461s\n",
      "Epoch: 0077 loss_train: 0.8299 acc_train: 0.7571 loss_val: 0.6138 acc_val: 0.8367 time: 4.8068s\n",
      "Epoch: 0078 loss_train: 0.6386 acc_train: 0.7929 loss_val: 0.6128 acc_val: 0.8367 time: 4.7181s\n",
      "Epoch: 0079 loss_train: 0.5410 acc_train: 0.8429 loss_val: 0.6123 acc_val: 0.8367 time: 4.8126s\n",
      "Epoch: 0080 loss_train: 0.6405 acc_train: 0.7714 loss_val: 0.6116 acc_val: 0.8367 time: 4.7789s\n",
      "Epoch: 0081 loss_train: 0.5964 acc_train: 0.8214 loss_val: 0.6102 acc_val: 0.8333 time: 4.7516s\n",
      "Epoch: 0082 loss_train: 0.6057 acc_train: 0.8071 loss_val: 0.6096 acc_val: 0.8300 time: 4.8002s\n",
      "Epoch: 0083 loss_train: 0.4802 acc_train: 0.8571 loss_val: 0.6092 acc_val: 0.8300 time: 4.7338s\n",
      "Epoch: 0084 loss_train: 0.6800 acc_train: 0.7500 loss_val: 0.6078 acc_val: 0.8300 time: 4.7445s\n",
      "Epoch: 0085 loss_train: 0.6114 acc_train: 0.8357 loss_val: 0.6068 acc_val: 0.8300 time: 4.7527s\n",
      "Epoch: 0086 loss_train: 0.5261 acc_train: 0.8071 loss_val: 0.6056 acc_val: 0.8300 time: 4.9145s\n",
      "Epoch: 0087 loss_train: 0.6018 acc_train: 0.7786 loss_val: 0.6045 acc_val: 0.8267 time: 4.7371s\n",
      "Epoch: 0088 loss_train: 0.6842 acc_train: 0.7857 loss_val: 0.6037 acc_val: 0.8267 time: 4.7527s\n",
      "Epoch: 0089 loss_train: 0.5571 acc_train: 0.8071 loss_val: 0.6024 acc_val: 0.8300 time: 4.7833s\n",
      "Epoch: 0090 loss_train: 0.5649 acc_train: 0.8071 loss_val: 0.6019 acc_val: 0.8333 time: 4.7501s\n",
      "Epoch: 0091 loss_train: 0.6126 acc_train: 0.8071 loss_val: 0.6014 acc_val: 0.8367 time: 4.7251s\n",
      "Epoch: 0092 loss_train: 0.7093 acc_train: 0.7357 loss_val: 0.6006 acc_val: 0.8367 time: 4.7838s\n",
      "Epoch: 0093 loss_train: 0.4388 acc_train: 0.8571 loss_val: 0.5997 acc_val: 0.8367 time: 4.8495s\n",
      "Epoch: 0094 loss_train: 0.5858 acc_train: 0.8286 loss_val: 0.5994 acc_val: 0.8367 time: 4.7811s\n",
      "Epoch: 0095 loss_train: 0.6697 acc_train: 0.7929 loss_val: 0.5992 acc_val: 0.8333 time: 4.7687s\n",
      "Epoch: 0096 loss_train: 0.5824 acc_train: 0.8143 loss_val: 0.5989 acc_val: 0.8333 time: 4.7862s\n",
      "Epoch: 0097 loss_train: 0.5544 acc_train: 0.8214 loss_val: 0.5985 acc_val: 0.8333 time: 4.8508s\n",
      "Epoch: 0098 loss_train: 0.5754 acc_train: 0.8357 loss_val: 0.5971 acc_val: 0.8333 time: 4.8043s\n",
      "Epoch: 0099 loss_train: 0.5891 acc_train: 0.8214 loss_val: 0.5959 acc_val: 0.8367 time: 4.7622s\n",
      "Epoch: 0100 loss_train: 0.4366 acc_train: 0.8500 loss_val: 0.5948 acc_val: 0.8367 time: 4.8005s\n",
      "Optimization Finished!\n",
      "Total time: 487.8709s\n",
      "Loading 99th epoch\n"
     ]
    },
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_total = time.time()\n",
    "loss_values = []\n",
    "bad_counter = 0\n",
    "best = args.epochs + 1\n",
    "best_epoch = 0\n",
    "for epoch in range(args.epochs):\n",
    "    loss_values.append(train(epoch))\n",
    "    torch.save(model.state_dict(), \"{}.pkl\".format(epoch))\n",
    "    if loss_values[-1] < best:\n",
    "        best = loss_values[-1]\n",
    "        best_epoch = epoch\n",
    "        bad_counter = 0\n",
    "    else:\n",
    "        bad_counter += 1\n",
    "    if bad_counter == args.patience:\n",
    "        break\n",
    "\n",
    "    files = glob.glob(\"*.pkl\")\n",
    "    for file in files:\n",
    "        epoch_nb = int(file.split(\".\")[0]) #\n",
    "        if epoch_nb < best_epoch:\n",
    "            os.remove(file)\n",
    "\n",
    "files = glob.glob('*.pkl')\n",
    "for file in files:\n",
    "    epoch_nb = int(file.split('.')[0])\n",
    "    if epoch_nb > best_epoch:\n",
    "        os.remove(file)\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time: {:.4f}s\".format(time.time() - t_total))\n",
    "print('Loading {}th epoch'.format(best_epoch))\n",
    "model.load_state_dict(torch.load('{}.pkl'.format(best_epoch)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(1, 4)\n",
      "tensor([1, 2, 3])\n",
      "tensor([2, 4])\n",
      "tensor([2, 3, 4])\n",
      "tensor([2, 3, 4])\n",
      "tensor([3, 5])\n"
     ]
    }
   ],
   "source": [
    "# 理解索引的使用\n",
    "a = torch.tensor([1, 2, 3, 4, 5])\n",
    "b = range(1, 4)\n",
    "c = torch.tensor(b)\n",
    "d = torch.tensor([2, 4])\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(a[b])\n",
    "print(a[c])\n",
    "print(a[d])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT(\n",
      "  (attention_0): GraphAttentionLayer(1433->8)\n",
      "  (attention_1): GraphAttentionLayer(1433->8)\n",
      "  (attention_2): GraphAttentionLayer(1433->8)\n",
      "  (attention_3): GraphAttentionLayer(1433->8)\n",
      "  (attention_4): GraphAttentionLayer(1433->8)\n",
      "  (attention_5): GraphAttentionLayer(1433->8)\n",
      "  (attention_6): GraphAttentionLayer(1433->8)\n",
      "  (attention_7): GraphAttentionLayer(1433->8)\n",
      "  (out_att): GraphAttentionLayer(64->7)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 理解加载模型的过程\n",
    "# # print(model)\n",
    "# # files = glob.glob('*.pkl')\n",
    "# # print(files)\n",
    "# checkpoint = torch.load(\"{}.pkl\".format(best_epoch), map_location=\"cpu\")\n",
    "# # print(checkpoint)\n",
    "# model.load_state_dict(checkpoint, strict=True)\n",
    "# print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}