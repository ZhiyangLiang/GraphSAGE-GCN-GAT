{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "\n",
    "class MeanAggregator(nn.Module):\n",
    "    def __init__(self, features, cuda=False, gcn=False):\n",
    "        super(MeanAggregator, self).__init__()\n",
    "        self.features = features\n",
    "        self.cuda = cuda\n",
    "        self.gcn = gcn\n",
    "\n",
    "    def forward(self, nodes, to_neighs, num_sample=10):\n",
    "        _set = set\n",
    "        if not num_sample is None:\n",
    "            _sample = random.sample\n",
    "            samp_neighs = [_set(_sample(to_neigh, num_sample)) if len(to_neigh) >= num_sample else to_neigh for to_neigh in to_neighs]\n",
    "            # 注意：此处先执行前面的判断语句，后执行for to_neigh in to_neighs形成list\n",
    "        else:\n",
    "            samp_neighs = to_neighs\n",
    "        if self.gcn:\n",
    "            samp_neighs = [set.union(samp_neigh, _set([nodes[i]])) for i, samp_neigh in enumerate(samp_neighs)]\n",
    "        unique_nodes_list = list(set.union(*samp_neighs))\n",
    "        unique_nodes = {n:i for i, n in enumerate(unique_nodes_list)}\n",
    "        mask = Variable(torch.zeros(len(samp_neighs), len(unique_nodes)))\n",
    "        column_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh]\n",
    "        row_indices = [i for i in range(len(samp_neighs)) for j in range(len(samp_neighs[i]))]\n",
    "        mask[row_indices, column_indices] = 1\n",
    "        if self.cuda:\n",
    "            mask = mask.cuda()\n",
    "        num_neigh = mask.sum(1, keepdims=True)\n",
    "        mask = mask.div(num_neigh)\n",
    "        if self.cuda:\n",
    "            embed_matrix = self.features(torch.LongTensor(unique_nodes_list).cuda())\n",
    "        else:\n",
    "            embed_matrix = self.features(torch.LongTensor(unique_nodes_list))\n",
    "        to_feats = mask.mm(embed_matrix)\n",
    "        return to_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{2, 3, 4}]\n"
     ]
    }
   ],
   "source": [
    "# 理解关于set的随机采样\n",
    "a = set\n",
    "b = random.sample\n",
    "c = [1,2,3,4,5]\n",
    "d = [a(b(c,3))]\n",
    "print(d)\n",
    "# print([a(c)+a(c)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{1, 2, 3, 4, 5}, {2, 3, 4, 5, 6}]\n",
      "[1, 2, 3, 4, 5, 6]\n",
      "{1, 2, 3, 4, 5, 7}\n"
     ]
    }
   ],
   "source": [
    "# 理解set的并集操作\n",
    "e = [2,3,4,5,6]\n",
    "f = [a(c), a(e)]\n",
    "print(f)\n",
    "g = list(set.union(*f))\n",
    "print(g)\n",
    "h = set.union(a(c), a([7]))\n",
    "print(h)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, features, feature_dim, embed_dim, adj_lists, aggregator, num_sample=10, base_model=None, gcn=False, cuda=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.features = features\n",
    "        self.feat_dim = feature_dim\n",
    "        self.adj_lists = adj_lists\n",
    "        self.aggregator = aggregator\n",
    "        self.num_sample = num_sample\n",
    "        if base_model != None:\n",
    "            self.base_model = base_model\n",
    "        self.gcn = gcn\n",
    "        self.embed_dim = embed_dim\n",
    "        self.cuda = cuda\n",
    "        self.aggregator.cuda = cuda\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(embed_dim, self.feat_dim if self.gcn else 2*self.feat_dim))\n",
    "        # 注意：此处将\",\"后面的语句作为整体执行\n",
    "        init.xavier_uniform_(self.weight) # 初始化操作\n",
    "\n",
    "    def forward(self, nodes):\n",
    "        # neigh_feats = self.aggregator.forward(nodes, self.adj_lists, self.num_sample)\n",
    "        neigh_feats = self.aggregator.forward(nodes, [self.adj_lists[int(node)] for node in nodes], self.num_sample)\n",
    "        if not self.gcn:\n",
    "            if self.cuda:\n",
    "                self_feats = self.features(torch.LongTensor(nodes).cuda())\n",
    "            else:\n",
    "                self_feats = self.features(torch.LongTensor(nodes))\n",
    "                # print(self_feats.shape) # (34, 16)\n",
    "            combined = torch.cat([self_feats, neigh_feats], dim=1)\n",
    "            # print(combined.shape) # (34, 32)\n",
    "        else:\n",
    "            combined = neigh_feats\n",
    "        # print(combined.shape) # (34, 32)\n",
    "        # print(self.weight.shape) # (16, 32)\n",
    "        combined = F.relu(self.weight.mm(combined.t()))\n",
    "        # print(combined.shape) # (16, 34)\n",
    "        return combined"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "from torch.nn import Linear\n",
    "\n",
    "class SupervisedGraphSage(nn.Module):\n",
    "    def __init__(self, num_classes, enc):\n",
    "        super(SupervisedGraphSage, self).__init__()\n",
    "        self.enc = enc\n",
    "        self.xent = nn.CrossEntropyLoss()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(num_classes, enc.embed_dim))\n",
    "        init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, nodes):\n",
    "        embeds = self.enc(nodes)\n",
    "        # print(embeds.shape) # (16, 34)\n",
    "        # print(self.weight.shape) # (4, 16)\n",
    "        scores = self.weight.mm(embeds)\n",
    "        # print(scores.shape) # (4, 34)\n",
    "        # return scores.t() # (34, 4)\n",
    "        scores_softmax = torch.exp(scores.t())/torch.sum(torch.exp(scores.t()), dim=1).reshape(-1, 1)\n",
    "        # print(scores_softmax.shape)\n",
    "        # print(scores_softmax.sum(dim=1))\n",
    "        return scores_softmax # (34, 4)\n",
    "\n",
    "    def loss(self, nodes, labels):\n",
    "        scores = self.forward(nodes)\n",
    "        # print(scores.sum(dim=1))\n",
    "        return self.xent(scores, labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "outputs": [],
   "source": [
    "def get_to_neighs(G):\n",
    "    num_nodes = len(G.nodes)\n",
    "    to_neighs = []\n",
    "    for i in range(num_nodes):\n",
    "        mid_set = set()\n",
    "        for j in G.neighbors(i):\n",
    "            mid_set.add(j)\n",
    "        to_neighs.append(mid_set)\n",
    "    return to_neighs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 3, 3, 3, 1, 0, 1, 3, 1, 1, 1, 0, 0, 3, 1, 0, 1, 0, 1, 0, 0,\n",
      "        2, 2, 0, 0, 2, 0, 0, 2, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# def get_batch_nodes(graph):\n",
    "#     batch_nodes = []\n",
    "#     num_nodes = len(graph.nodes)\n",
    "#     pos = nx.spring_layout(graph)\n",
    "#     for i in range(num_nodes):\n",
    "#         batch_nodes.append(list(pos[i]))\n",
    "#     return batch_nodes\n",
    "\n",
    "G = nx.karate_club_graph()\n",
    "# batch_nodes = get_batch_nodes(G)\n",
    "# print(batch_nodes)\n",
    "\n",
    "from torch_geometric.datasets import KarateClub\n",
    "dataset = KarateClub()\n",
    "data = dataset[0]\n",
    "labels = data.y\n",
    "print(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "outputs": [],
   "source": [
    "def create_elements(graph = nx.karate_club_graph(), num_embed = 34, feature_dim = 16, embed_dim = 16, num_classes = 4, nodes=None):\n",
    "    if nodes == None:\n",
    "        nodes = []\n",
    "        for i in range(len(graph.nodes)):\n",
    "            nodes.append(i)\n",
    "    split = int(len(nodes)/3*2)\n",
    "    features = nn.Embedding(num_embed, embed_dim)\n",
    "    to_neighs = get_to_neighs(graph)\n",
    "    aggregator = MeanAggregator(features, nodes[:split], to_neighs)\n",
    "    enc = Encoder(features=features, feature_dim=feature_dim, embed_dim=embed_dim, adj_lists=to_neighs, aggregator=aggregator)\n",
    "    model = SupervisedGraphSage(num_classes=num_classes, enc=enc)\n",
    "    # criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    return nodes, to_neighs, model, optimizer, features, split\n",
    "\n",
    "def train(nodes, labels, model, optimizer):\n",
    "    times = []\n",
    "    losses = []\n",
    "    for i in range(100):\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(nodes, labels)\n",
    "        losses.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        end_time = time.time()\n",
    "        times.append(end_time-start_time)\n",
    "    # print(times)\n",
    "    # print(losses)\n",
    "\n",
    "def predict(nodes, labels, model):\n",
    "    after_model = model(nodes)\n",
    "    predicted = torch.argmax(after_model, dim=1)\n",
    "    acc = (torch.sum(predicted == labels) / len(labels)).item()\n",
    "    print(acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "数据集：nx.karate_club_graph()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4166666567325592\n"
     ]
    }
   ],
   "source": [
    "nodes, to_neights, model, optimizer, features,split = create_elements(graph = nx.karate_club_graph(), num_embed = 34, feature_dim = 16, embed_dim = 16, num_classes = 4, nodes=None)\n",
    "train(nodes=nodes[:split], labels=labels[:split], model=model, optimizer=optimizer)\n",
    "predict(nodes=nodes[split:], labels=labels[split:], model=model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "数据集：nx.read_edgelist(\"./facebook/414.edges\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 1693\n",
      "0.9800000190734863\n"
     ]
    }
   ],
   "source": [
    "G_fb = nx.read_edgelist(\"./facebook/414.edges\")\n",
    "n = G_fb.number_of_nodes()\n",
    "m = G_fb.number_of_edges()\n",
    "print(n, m)\n",
    "mapping = dict(zip(G_fb.nodes(), range(n)))\n",
    "# print(mapping)\n",
    "nx.relabel_nodes(G_fb, mapping, copy=False)\n",
    "# print(G_fb.nodes)\n",
    "\n",
    "nodes_fb, to_neights_fb, model_fb, optimizer_fb, features_fb, split_fb = create_elements(graph = G_fb, num_embed = 150, feature_dim = 16, embed_dim = 16, num_classes = 11, nodes=list(G_fb.nodes))\n",
    "labels_fb = torch.tensor([random.randint(0, 10) for i in range(150)])\n",
    "train(nodes=nodes_fb[:split_fb], labels=labels_fb[:split_fb], model=model_fb, optimizer=optimizer_fb)\n",
    "predict(nodes=nodes_fb[:split_fb], labels=labels_fb[:split_fb], model=model_fb)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "数据集：cora"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([34, 128])\n",
      "0 1.9485934972763062\n",
      "1 1.947161316871643\n",
      "2 1.944930911064148\n",
      "3 1.9428558349609375\n",
      "4 1.9426424503326416\n",
      "5 1.9395396709442139\n",
      "6 1.9390537738800049\n",
      "7 1.9382331371307373\n",
      "8 1.9358720779418945\n",
      "9 1.9325929880142212\n",
      "10 1.933243989944458\n",
      "11 1.930747389793396\n",
      "12 1.9301179647445679\n",
      "13 1.924185872077942\n",
      "14 1.921176791191101\n",
      "15 1.9207282066345215\n",
      "16 1.9116379022598267\n",
      "17 1.9020146131515503\n",
      "18 1.8978430032730103\n",
      "19 1.8932890892028809\n",
      "20 1.875340223312378\n",
      "21 1.8655987977981567\n",
      "22 1.8482781648635864\n",
      "23 1.8441038131713867\n",
      "24 1.836130142211914\n",
      "25 1.8032073974609375\n",
      "26 1.806523323059082\n",
      "27 1.7950505018234253\n",
      "28 1.7865225076675415\n",
      "29 1.7405025959014893\n",
      "30 1.7768893241882324\n",
      "31 1.8011536598205566\n",
      "32 1.7608423233032227\n",
      "33 1.7367984056472778\n",
      "34 1.7544971704483032\n",
      "35 1.7346073389053345\n",
      "36 1.7169190645217896\n",
      "37 1.7079381942749023\n",
      "38 1.732297658920288\n",
      "39 1.683693528175354\n",
      "40 1.6918920278549194\n",
      "41 1.672461748123169\n",
      "42 1.6427375078201294\n",
      "43 1.698271632194519\n",
      "44 1.6535955667495728\n",
      "45 1.660947322845459\n",
      "46 1.663649320602417\n",
      "47 1.6362038850784302\n",
      "48 1.6415208578109741\n",
      "49 1.6519876718521118\n",
      "50 1.6229654550552368\n",
      "51 1.6639518737792969\n",
      "52 1.6455036401748657\n",
      "53 1.6308034658432007\n",
      "54 1.6274257898330688\n",
      "55 1.6390482187271118\n",
      "56 1.6397876739501953\n",
      "57 1.620708703994751\n",
      "58 1.5885990858078003\n",
      "59 1.5974925756454468\n",
      "60 1.6435312032699585\n",
      "61 1.666006088256836\n",
      "62 1.6044408082962036\n",
      "63 1.5702875852584839\n",
      "64 1.6088351011276245\n",
      "65 1.5860782861709595\n",
      "66 1.5976018905639648\n",
      "67 1.5541067123413086\n",
      "68 1.547734022140503\n",
      "69 1.5740065574645996\n",
      "70 1.5378040075302124\n",
      "71 1.5648919343948364\n",
      "72 1.536824107170105\n",
      "73 1.5040487051010132\n",
      "74 1.5363205671310425\n",
      "75 1.5260838270187378\n",
      "76 1.5081995725631714\n",
      "77 1.472034215927124\n",
      "78 1.5019265413284302\n",
      "79 1.5154449939727783\n",
      "80 1.4649903774261475\n",
      "81 1.4784237146377563\n",
      "82 1.4435755014419556\n",
      "83 1.427695393562317\n",
      "84 1.42865788936615\n",
      "85 1.444979190826416\n",
      "86 1.432743787765503\n",
      "87 1.4181166887283325\n",
      "88 1.392835259437561\n",
      "89 1.4108551740646362\n",
      "90 1.3964810371398926\n",
      "91 1.3905696868896484\n",
      "92 1.4187452793121338\n",
      "93 1.375958800315857\n",
      "94 1.3880022764205933\n",
      "95 1.404245138168335\n",
      "96 1.360959529876709\n",
      "97 1.3706244230270386\n",
      "98 1.3860037326812744\n",
      "99 1.376800775527954\n",
      "Validation F1: 0.802\n",
      "Average batch time: 0.08406116485595704\n"
     ]
    }
   ],
   "source": [
    "def load_cora():\n",
    "    num_nodes = 2708\n",
    "    num_feats = 1433\n",
    "    feat_data = np.zeros((num_nodes, num_feats)) # 此处不能用torch代替np\n",
    "    labels = np.empty((num_nodes, 1), dtype=np.int64) # 此处不能用torch代替np\n",
    "    node_map = {}\n",
    "    label_map = {}\n",
    "    with open(\"./cora/cora.content\") as fp:\n",
    "        for i, line in enumerate(fp):\n",
    "            info = line.strip().split()\n",
    "            info_label = info[-1]\n",
    "            info = info[:-1]\n",
    "            info = [int(x) for x in info]\n",
    "            feat_data[i, :] = info[1:]\n",
    "            # print(len(feat_data[i, :]))\n",
    "            node_map[info[0]] = i\n",
    "            if not info_label in label_map:\n",
    "                label_map[info_label] = len(label_map) # len({}) = 1\n",
    "            labels[i] = label_map[info_label]\n",
    "    adj_lists = defaultdict(set) # 注意：此处不能用普通的set\n",
    "    # print(adj_lists)\n",
    "    # print(node_map)\n",
    "    # print(label_map)\n",
    "    # print(labels.shape)\n",
    "    with open(\"./cora/cora.cites\") as fp:\n",
    "        for i, line in enumerate(fp):\n",
    "            info = line.strip().split()\n",
    "            info = [int(x) for x in info]\n",
    "            paper1 = node_map[info[0]]\n",
    "            paper2 = node_map[info[1]]\n",
    "            adj_lists[paper1].add(paper2)\n",
    "            adj_lists[paper2].add(paper1)\n",
    "    return feat_data, labels, adj_lists\n",
    "\n",
    "# def run_cora():\n",
    "np.random.seed(1) # 应对用np.random生成的随机数\n",
    "random.seed(1) # 应对用random生成的随机数\n",
    "num_nodes = 2708\n",
    "num_feats = 1433\n",
    "feat_data, labels, adj_lists = load_cora()\n",
    "features = nn.Embedding(num_nodes, num_feats) # torch.LongTensor等价于torch.tensor\n",
    "features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=False) # torch.FloatTensor等价于torch.Tensor\n",
    "# 上一行的作用：加快训练速度，提升训练效果(F1)\n",
    "# print(features)\n",
    "# print(features.weight.sum(dim=1))\n",
    "\n",
    "agg1 = MeanAggregator(features)\n",
    "enc1 = Encoder(features=features, feature_dim=1433, embed_dim=128, adj_lists=adj_lists, aggregator=agg1) # gcn=False\n",
    "print(enc1(nodes).t().shape)\n",
    "agg2 = MeanAggregator(features=lambda nodes : enc1(nodes).t())\n",
    "enc2 = Encoder(features=lambda nodes : enc1(nodes).t(), feature_dim=enc1.embed_dim, embed_dim=128, adj_lists=adj_lists, aggregator=agg2, base_model=enc1) # gcn=False\n",
    "\n",
    "graphsage = SupervisedGraphSage(7, enc2)\n",
    "rand_indices = np.random.permutation(num_nodes)\n",
    "# print(num_nodes)\n",
    "# print(rand_indices)\n",
    "test = rand_indices[:1000]\n",
    "val = rand_indices[1000:1500]\n",
    "train = list(rand_indices[1500:])\n",
    "optimizer = torch.optim.SGD(filter(lambda p : p.requires_grad, graphsage.parameters()), lr=0.7)\n",
    "# parameters = filter(lambda p : p.requires_grad, graphsage.parameters())\n",
    "# print(parameters)\n",
    "# print(graphsage.parameters())\n",
    "times = []\n",
    "for batch in range(100):\n",
    "    batch_nodes = train[:256]\n",
    "    random.shuffle(train)\n",
    "    start_time = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    loss = graphsage.loss(batch_nodes, torch.LongTensor(labels[np.array(batch_nodes)].reshape(1, -1)[0]))# Variable\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    end_time = time.time()\n",
    "    times.append(end_time-start_time)\n",
    "    print(batch, loss.item())\n",
    "\n",
    "# print(labels[val])\n",
    "# print(labels[train]) # 二者在格式上没有区别\n",
    "val_output = graphsage.forward(val)\n",
    "# print(val_output.data.numpy().argmax(axis=1))\n",
    "# print(\"-\"*50)\n",
    "# print(val_output.argmax(axis=1))\n",
    "print(\"Validation F1:\", f1_score(labels[val], val_output.argmax(axis=1), average=\"micro\"))\n",
    "print(\"Average batch time:\", np.mean(times))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "下面均为帮助理解整个网络架构的测试代码"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "to_neighs = get_to_neighs(G)\n",
    "samp_neighs = [set(random.sample(to_neigh, 3)) if len(to_neigh) >= 3 else to_neigh for to_neigh in to_neighs]\n",
    "# print(samp_neighs)\n",
    "samp_neighs = [set.union(samp_neigh, set([nodes[i]])) for i, samp_neigh in enumerate(samp_neighs)]\n",
    "# print(samp_neighs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unique_nodes_list = list(set.union(*samp_neighs))\n",
    "# print(unique_nodes_list)\n",
    "unique_nodes = {n:i for i, n in enumerate(unique_nodes_list)}\n",
    "# print(unique_nodes)\n",
    "mask = Variable(torch.zeros(len(samp_neighs), len(unique_nodes)))\n",
    "column_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh]\n",
    "row_indices = [i for i in range(len(samp_neighs)) for j in range(len(samp_neighs[i]))]\n",
    "mask[row_indices, column_indices] = 1\n",
    "num_neigh = mask.sum(1, keepdims=True)\n",
    "mask = mask.div(num_neigh) # 对每个节点的影响权权重(此处为mean)，节点自身与采样节点的权重一致"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embed_matrix = features(torch.LongTensor(unique_nodes_list)) # 34个节点，每个节点都有唯一的编码\n",
    "# print(embed_matrix)\n",
    "to_feats = mask.mm(embed_matrix)\n",
    "# print(mask.shape)\n",
    "# print(embed_matrix.shape)\n",
    "# print(to_feats.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "weight = nn.Parameter(torch.FloatTensor(3, 2 if False else 4))\n",
    "init.xavier_uniform_(weight)\n",
    "# print(weight)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "self_feats = features(torch.LongTensor(nodes))\n",
    "# print(self_feats.shape)\n",
    "# print(features)\n",
    "# print(torch.LongTensor(nodes))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a = \"asd qwert zxcv\"\n",
    "a = a.strip().split()\n",
    "print(map(float, a[1:-1]))\n",
    "c = np.zeros((3,3))\n",
    "c[1,:]=map(float, a[1:-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}